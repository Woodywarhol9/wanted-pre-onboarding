{"cells":[{"cell_type":"markdown","metadata":{"id":"592U6lXs3d2t"},"source":["# Week2_2 Assignment\n","\n","## [BASIC](#Basic) \n","- \"네이버 영화 감성 분류\" 데이터를 불러와 `pandas` 라이브러리를 사용해 **전처리** 할 수 있다.\n","- 적은 데이터로도 높은 성능을 내기 위해, pre-trained `BERT` 모델 위에 1개의 hidden layer를 쌓아 **fine-tuning**할 수 있다.\n","\n","## [CHALLENGE](#Challenge)\n","- 토큰화된 학습 데이터를 배치 단위로 갖는 **traindata iterator**를 구현할 수 있다. \n","\n","## [ADVANCED](#Advanced)\n","- **loss와 optimizer 함수**를 사용할 수 있다. \n","- traindata iterator를 for loop 돌며 **fine-tuning** 할 수 있다.\n","- fine-tuning의 2가지 방법론을 비교할 수 있다. \n","  - BERT 파라미터를 **freeze** 한 채 fine-tuning (Vision에서 주로 사용하는 방법론)\n","  - BERT 파라미터를 **unfreeze** 한 채 fine-tuning (NLP에서 주로 사용하는 방법론)\n","\n","\n","### Reference\n","- [huggingface 한국어 오픈소스 모델](https://huggingface.co/models?language=ko&sort=downloads&search=bert)\n","- [transformer BertForSequenceClassification 소스 코드](https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert/modeling_bert.py#L1501)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"KSX-wQA1RD1h"},"outputs":[],"source":["import os\n","import sys\n","import pandas as pd\n","import numpy as np \n","import torch\n","import random"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"4Reyt-HvLnJv"},"outputs":[],"source":["# seed\n","seed = 7777\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"gUR6vb3L3d2u"},"outputs":[{"name":"stdout","output_type":"stream","text":["# available GPUs : 1\n","GPU name : Graphics Device\n","cuda\n"]}],"source":["# device type\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","  print(f\"# available GPUs : {torch.cuda.device_count()}\")\n","  print(f\"GPU name : {torch.cuda.get_device_name()}\")\n","else:\n","  device = torch.device(\"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"c93M8XmjLnJw"},"source":["## Basic"]},{"cell_type":"markdown","metadata":{"id":"0REKl4EvT9G1"},"source":["### 데이터 다운로드 및 DataFrame 형태로 불러오기\n","- 내 구글 드라이브에 데이터를 다운받은 후 코랩에 드라이브를 마운트하면 데이터를 영구적으로 사용할 수 있음.\n","- [네이버영화감성분류](https://github.com/e9t/nsmc)\n","  - trainset: 150,000 \n","  - testset: 50,000 "]},{"cell_type":"markdown","metadata":{},"source":["### !!colab gpu 사용량 초과로 로컬에서 실행했습니다!!"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"lEWUggR1R9rS"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"rov1s8IxSLqy"},"outputs":[],"source":["#cd \"/content/drive/MyDrive/여기에 파일 경로를 입력\""]},{"cell_type":"code","execution_count":6,"metadata":{"id":"OjPGnbEjVYmj"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'nsmc' already exists and is not an empty directory.\n"]}],"source":["# 데이터 다운로드\n","!git clone https://github.com/e9t/nsmc.git"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[01;34m.\u001b[00m\n","├── \u001b[01;34m./nsmc\u001b[00m\n","│   ├── \u001b[01;34m./nsmc/code\u001b[00m\n","│   ├── ./nsmc/ratings_test.txt\n","│   ├── ./nsmc/ratings_train.txt\n","│   ├── ./nsmc/ratings.txt\n","│   ├── \u001b[01;34m./nsmc/raw\u001b[00m\n","│   ├── ./nsmc/README.md\n","│   └── ./nsmc/synopses.json\n","└── ./Week2_2_assignment.ipynb\n","\n","3 directories, 6 files\n"]}],"source":["!tree -L 2 -f"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"SueG9v14YbgF"},"outputs":[{"name":"stdout","output_type":"stream","text":["My current directory : /home/wjs/workspace/pre_onboarding\n"]}],"source":["_CUR_DIR = os.path.abspath(os.curdir)\n","print(f\"My current directory : {_CUR_DIR}\")\n","_DATA_DIR = os.path.join(_CUR_DIR, \"nsmc\")"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"9J6KQ8dzaHBi"},"outputs":[],"source":["# nsmc/ratings_train.txt를 DataFrame 형태로 불러오기\n","df_train_path = os.path.join(_DATA_DIR, \"ratings_train.txt\") #df_train_path\n","df = pd.read_csv(df_train_path, sep = \"\\t\") "]},{"cell_type":"code","execution_count":10,"metadata":{"id":"3cUsoBEPahlo"},"outputs":[{"data":{"text/plain":["(150000, 3)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# 데이터 크기 확인\n","df.shape"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Ic3k9CORaXzM"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9976970</td>\n","      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3819312</td>\n","      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10265843</td>\n","      <td>너무재밓었다그래서보는것을추천한다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9045019</td>\n","      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6483659</td>\n","      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id                                           document  label\n","0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n","1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n","2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n","3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n","4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# 데이터 일부 확인\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"JA1F0tHWLnJz"},"source":["### 데이터 결측치 제거 및 데이터 수 줄이기 \n","- 학습 데이터 수는 150,000개로 매우 많은 양이다. 하지만 우리가 실생활에서 마주할 데이터는 이렇게 많지 않다. 이 때 유용하게 사용되는 것이 **fine-tuning** 학습 방법이다.   \n","- Fine-tuning은 단어의 의미를 이미 충분히 학습한 모델 (여기서는 **BERT**)을 가져와 그 위에 추가적인 Nueral Network 레이어를 쌓은 후 학습하는 방법론이다. 이미 BERT가 단어의 의미를 충분히 학습했기 때문에 **적은 데이터**로 학습해도 우수한 성능을 낼 수 있다는 장점이 있다. \n","- **데이터의 label의 비율이 5:5를 유지하면서** 학습 데이터 수를 150,000개에서 1,000개로 줄이\b는 함수 `label_evenly_balanced_dataset_sampler`를 구현하라.\n","  - 함수 정의 \n","    - 입력 매개변수\n","      - df : DataFrame\n","      - n_sample : df에서 샘플링할 row의 개수 (여기서는 1000개로 정의한다)\n","    - 조건\n","      - label의 비율이 5:5를 유지할 수 있도록 샘플링한다.\n","    - 반환값\n","      - row의 개수가 1000개인 dataframe"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 150000 entries, 0 to 149999\n","Data columns (total 3 columns):\n"," #   Column    Non-Null Count   Dtype \n","---  ------    --------------   ----- \n"," 0   id        150000 non-null  int64 \n"," 1   document  149995 non-null  object\n"," 2   label     150000 non-null  int64 \n","dtypes: int64(2), object(1)\n","memory usage: 3.4+ MB\n"]}],"source":["#df_info check\n","df.info()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>25857</th>\n","      <td>2172111</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>55737</th>\n","      <td>6369843</td>\n","      <td>NaN</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>110014</th>\n","      <td>1034280</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>126782</th>\n","      <td>5942978</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>140721</th>\n","      <td>1034283</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id document  label\n","25857   2172111      NaN      1\n","55737   6369843      NaN      1\n","110014  1034280      NaN      0\n","126782  5942978      NaN      0\n","140721  1034283      NaN      0"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["#df['documnet] null check\n","df.loc[df['document'].isnull()]"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Lh9BSiSeMms7"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9976970</td>\n","      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3819312</td>\n","      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10265843</td>\n","      <td>너무재밓었다그래서보는것을추천한다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9045019</td>\n","      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6483659</td>\n","      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>149995</th>\n","      <td>6222902</td>\n","      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>149996</th>\n","      <td>8549745</td>\n","      <td>평점이 너무 낮아서...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>149997</th>\n","      <td>9311800</td>\n","      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>149998</th>\n","      <td>2376369</td>\n","      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>149999</th>\n","      <td>9619869</td>\n","      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>149995 rows × 3 columns</p>\n","</div>"],"text/plain":["              id                                           document  label\n","0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n","1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n","2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n","3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n","4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n","...          ...                                                ...    ...\n","149995   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n","149996   8549745                                      평점이 너무 낮아서...      1\n","149997   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n","149998   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n","149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n","\n","[149995 rows x 3 columns]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# df에서 결측치 (na 값) 제거\n","df.dropna(axis = 0, inplace=True)\n","df"]},{"cell_type":"markdown","metadata":{},"source":["0(Negative) : 75170   \n","1(Positive) : 74825"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"ommF5KH4akCJ"},"outputs":[{"data":{"text/plain":["0    75170\n","1    74825\n","Name: label, dtype: int64"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# label별 데이터 수 확인\n","# pandas의 value_counts 함수 활용\n","# 0 -> 부정 1 -> 긍정\n","df['label'].value_counts() "]},{"cell_type":"code","execution_count":16,"metadata":{"id":"_ii06wCsc107"},"outputs":[],"source":["# 학습 데이터 샘플 개수 설정\n","n_sample = 1000"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Hrkhl69Dc-kr"},"outputs":[],"source":["# 샘플링 함수 구현\n","# random 모듈에서 제공되는 함수 활용\n","# input: 학습 데이터 샘플 개수\n","# output: 샘플링 데이터\n","\n","\n","def label_evenly_balanced_dataset_sampler(df, sample_size):\n","  \"\"\"\n","  데이터 프레임의을 sample_size만큼 임의 추출해 새로운 데이터 프레임을 생성.\n","  이 때, \"label\"열의 값들이 동일한 비율을 갖도록(5:5) 할 것.\n","  \"\"\"\n","  df_p = df.loc[df['label'] == 1].sample(n = int(sample_size / 2), random_state = 42) #label == 1 추출 후 sample\n","  df_n = df.loc[df['label'] == 0].sample(n = int(sample_size / 2), random_state = 42) #label == 0 추출 후 sample\n","\n","  # df_p, df_n concat 후 새로 생성되는 index col drop\n","  sample = pd.concat([df_p, df_n]).reset_index(drop = True) \n","\n","  return sample\n","\n","sample_df = label_evenly_balanced_dataset_sampler(df, n_sample)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2226032</td>\n","      <td>전개가 어색하지만 유코의 매력에 빠져들게 되는..단발머리 너무 어울려..</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>9236120</td>\n","      <td>꽃보다청춘 라오스편 너무 재밌게 보고있어요 시리즈중 제일잘만들었어요 노래도 넘 좋구...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10049441</td>\n","      <td>이런 꿀잼영화는 더 많은 사람들이 봐야한다</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7494607</td>\n","      <td>인상 깊은 드라마</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5730262</td>\n","      <td>두 배우가 연기하느라 애썼다. 곳곳 명대사 덕에 속상한 마음이 좀 누그러졌음_.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>3184956</td>\n","      <td>라스트씬은 괜찮았다.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>8673948</td>\n","      <td>이 시간대 볼만한 드라마도 없고 이서진 얼굴볼라고 참고 보는데 진짜 너무 재미없고 ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>4711989</td>\n","      <td>사실....너무 허접하다고 느껴지는건 사실이다.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>9082735</td>\n","      <td>\"테레사 수녀는 지옥에 있다 ..\"\" 테레사 수녀의 가증스런 \"\" 이라고 검색해서 ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>9060870</td>\n","      <td>댓글보니까 다 알바네 거의다 조카랑 보러갔뎈ㅋㅋ</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 3 columns</p>\n","</div>"],"text/plain":["           id                                           document  label\n","0     2226032           전개가 어색하지만 유코의 매력에 빠져들게 되는..단발머리 너무 어울려..      1\n","1     9236120  꽃보다청춘 라오스편 너무 재밌게 보고있어요 시리즈중 제일잘만들었어요 노래도 넘 좋구...      1\n","2    10049441                            이런 꿀잼영화는 더 많은 사람들이 봐야한다      1\n","3     7494607                                          인상 깊은 드라마      1\n","4     5730262       두 배우가 연기하느라 애썼다. 곳곳 명대사 덕에 속상한 마음이 좀 누그러졌음_.      1\n","..        ...                                                ...    ...\n","995   3184956                                        라스트씬은 괜찮았다.      0\n","996   8673948  이 시간대 볼만한 드라마도 없고 이서진 얼굴볼라고 참고 보는데 진짜 너무 재미없고 ...      0\n","997   4711989                         사실....너무 허접하다고 느껴지는건 사실이다.      0\n","998   9082735  \"테레사 수녀는 지옥에 있다 ..\"\" 테레사 수녀의 가증스런 \"\" 이라고 검색해서 ...      0\n","999   9060870                         댓글보니까 다 알바네 거의다 조카랑 보러갔뎈ㅋㅋ      0\n","\n","[1000 rows x 3 columns]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["sample_df"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"hXLT6tAdaA34"},"outputs":[{"data":{"text/plain":["1    500\n","0    500\n","Name: label, dtype: int64"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# 검증\n","sample_df.label.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"cLNUjgawLnJ1"},"source":["### CustomClassifier 클래스 구현\n","<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week2/week2_2_bertclf.png?raw=true\" width=400>\n","\n","- 그림과 같이 사전 학습(pre-trained)된 `BERT` 모델을 불러와 그 위에 **1 hidden layer**와 **binary classifier layer**를 쌓아 fine-tunning 모델을 생성할 것이다.    \n","---\n","- hidden layer 1개와 output layer(binary classifier layer)를 갖는 `CustomClassifier` 클래스를 구현하라.\n","- 클래스 정의\n","  - 생성자 입력 매개변수\n","    - `hidden_size` : BERT의 embedding size\n","    - `n_label` : class(label) 개수\n","  - 생성자에서 생성할 변수\n","    - `bert` : BERT 모델 인스턴스 \n","    - `classifier` : 1 hidden layer + relu +  dropout + classifier layer를 stack한 `nn.Sequential` 모델\n","      - 첫번재 히든 레이어 (첫번째 `nn.Linear`)\n","        - input: BERT의 마지막 layer의 1번재 token ([CLS] 토큰) (shape: `hidden_size`)\n","        - output: (shape: `linear_layer_hidden_size`)\n","      - 아웃풋 레이어 (두번째 `nn.Linear`)\n","        - input: 첫번째 히든 레이어의 아웃풋 (shape: `linear_layer_hidden_size`)\n","        - output: target/label의 개수 (shape:2)\n","  - 메소드\n","    - `forward()`\n","      - BERT output에서 마지막 레이어의 첫번째 토큰 ('[CLS]')의 embedding을 가져와 `self.classifier`에 입력해 아웃풋으로 logits를 출력함.\n","  - 주의 사항\n","    - `CustomClassifier` 클래스는 부모 클래스로 `nn.Module`을 상속 받는다.\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"U0WbqVv62Zvy"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"Im98H4-U1eQQ"},"outputs":[],"source":["# classifier 구현\n","class CustomClassifier(nn.Module):\n","\n","  def __init__(self, hidden_size: int, n_label: int):\n","    super(CustomClassifier, self).__init__() \n","\n","    self.bert = BertModel.from_pretrained(\"klue/bert-base\") #bert model instance\n","\n","    dropout_rate = 0.1\n","    linear_layer_hidden_size = 32\n","\n","    self.classifier = nn.Sequential(\n","      nn.Linear(hidden_size, linear_layer_hidden_size), # input shape : hidden_size, output shape : 32(linear_layer_hidden_size)\n","      nn.ReLU(),\n","      nn.Dropout(p = dropout_rate),\n","      nn.Linear(linear_layer_hidden_size, n_label) # input shape : 32, output shape : 2(n_label)  \n","      ) # torch.nn에서 제공되는 Sequential, Linear, ReLU, Dropout 함수 활용\n","\n","\n","  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n","\n","    outputs = self.bert(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        token_type_ids=token_type_ids,\n","    )\n","\n","    # BERT 모델의 마지막 레이어의 첫번재 토큰을 인덱싱\n","    cls_token_last_hidden_states = outputs['pooler_output'] # 마지막 layer의 첫 번째 토큰 (\"[CLS]\") 벡터를 가져오기, shape = (1, hidden_size)\n","\n","    logits = self.classifier(cls_token_last_hidden_states)\n","\n","    return logits"]},{"cell_type":"markdown","metadata":{"id":"9x7PU1t1LnJ1"},"source":["## Challenge"]},{"cell_type":"markdown","metadata":{"id":"YXesCG5TLnJ1"},"source":["### 학습 데이터를 배치 단위로 저장하는 이터레이터 함수 `data_iterator` 구현\n","- 데이터 프레임을 입력 받아 text를 \b토큰 id로 변환하고 label은 텐서로 변환해 배치만큼 잘라 (input, \btarget) 튜플 형태의 이터레이터를 생성하는 `data_iterator` 함수를 구현하라.\n","- 함수 정의 \n","  - 입력 매개변수\n","    - `input_column` : text 데이터 column 명\n","    - `target_column` : label 데이터 column 명\n","    -  `batch_size` : 배치 사이즈\n","  - 조건\n","    - 함수는 다음을 수행해야 함 \n","      - 데이터 프레임 랜덤 셔플링\n","      - `tokenizer_bert`로 text를 token_id로 변환 + 텐서화 \n","      - target(label)을 텐서화\n","  - 반환값 \n","    - (input, target) 튜플 형태의 이터레이터를 반환"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"q-tJERGI4Fzk","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /home/wjs/anaconda3/lib/python3.9/site-packages (4.16.2)\n","Requirement already satisfied: packaging>=20.0 in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (21.0)\n","Requirement already satisfied: pyyaml>=5.1 in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (2.26.0)\n","Requirement already satisfied: regex!=2019.12.17 in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (2021.8.3)\n","Requirement already satisfied: numpy>=1.17 in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (0.11.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.0)\n","Requirement already satisfied: sacremoses in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (0.0.47)\n","Requirement already satisfied: filelock in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (3.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (4.62.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wjs/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /home/wjs/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/wjs/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /home/wjs/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /home/wjs/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/wjs/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: click in /home/wjs/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.3)\n","Requirement already satisfied: joblib in /home/wjs/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /home/wjs/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.15.0)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"JlcYCOyW3d2t"},"outputs":[],"source":["from transformers import BertTokenizer, BertModel"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"C_U_c-Mf3d2t"},"outputs":[],"source":["tokenizer_bert = BertTokenizer.from_pretrained(\"klue/bert-base\") # lower-cased version"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"p2VnIY-ALnJ2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Sentence: 전개가 어색하지만 유코의 매력에 빠져들게 되는..단발머리 너무 어울려..\n","\n","Tokenized Sentence: {'input_ids': tensor([[    2,  5445,  2116,  9977,  2205,  3683,  1490,  2258,  2079,  5147,\n","          2170, 16889,  2318,   859,  2259,    18,    18, 19690,  6536,  3760,\n","         13486,    18,    18,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"]}],"source":["# 토크나이징 예시 (1개의 문장)\n","\n","# 1. string type의 문장을 가져옴\n","ex_sent = sample_df.document.iloc[0]\n","print(f\"Original Sentence: {ex_sent}\\n\")\n","\n","# 2. 문장을 토크나이즈 함. 이 때, 특수 토큰 (\"[CLS]\", \"[SPE]\")을 자동으로 추가하고 pytorch의 tensor형태로 변환해 반환함\n","tensor_sent = tokenizer_bert(\n","    ex_sent,\n","    add_special_tokens=True, # 문장의 앞에 문장 시작을 알리는 \"[CLS]\"토큰, 문장의 끝에 문장 끝을 알리는 \"[SPE]\"토큰을 자동으로 추가\n","    return_tensors='pt' # pytorch tensor로 반환할 것\n",")\n","print(f\"Tokenized Sentence: {tensor_sent}\")"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"rtXP_wRFLnJ2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Sentence 1: 전개가 어색하지만 유코의 매력에 빠져들게 되는..단발머리 너무 어울려..\n","Original Sentence 2: 꽃보다청춘 라오스편 너무 재밌게 보고있어요 시리즈중 제일잘만들었어요 노래도 넘 좋구요~ 앞으로도 연석호준바로 이멤버로 계속 여행 찍었으면 좋겠네요\n","\n","Tokenized Sentence list: {'input_ids': tensor([[    2,  5445,  2116,  9977,  2205,  3683,  1490,  2258,  2079,  5147,\n","          2170, 16889,  2318,   859,  2259,    18,    18, 19690,  6536,  3760,\n","         13486,    18,    18,     3,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0],\n","        [    2,   684,  2178,  2062,  2270,  2363, 18389,  2578,  3760,  7478,\n","          2318,  4530,  2689, 10283,  5526,  2284,  4920,  2917,  2154,  2031,\n","          2359, 10283,  4388,  2119,   749,  1560,  5515,    97,  1388,  6233,\n","          2119, 18783,  2016,  2456,  5856,  1504,  3333,  2264,  2200,  3851,\n","          4062,  1626,  2359,  6076,  1560,  2918,  2203,  2182,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1]])}\n"]}],"source":["# 토크나이징 예시 (2개의 문장)\n","\n","# 1. 2개의 문장을 가진 list 생성\n","ex_sent_list = list(sample_df.document.iloc[:2].values)\n","for i, sent in enumerate(ex_sent_list):\n","    print(f\"Original Sentence {i+1}: {sent}\")\n","\n","# 2. 문장 리스트를 토크나이즈 함. 이 때, 리스트 내 문장들의 토큰 길이가 동일할 수 있도록 가장 긴 문장을 기준으로 부족한 위치에 \"[PAD]\" 토큰을 추가\n","tensor_sent_list = tokenizer_bert(\n","    ex_sent_list,\n","    add_special_tokens=True,\n","    return_tensors='pt',\n","    padding=\"longest\" # 가장 긴 문장을 기준으로 token개수를 맞춤. 모자란 토큰 위치는 \"[PAD]\" 토큰을 추가\n",")\n","\n","print(f\"\\nTokenized Sentence list: {tensor_sent_list}\")\n","\n","# 토크나이즈 된 두 문장의 길이가 동일함을 검증\n","assert tensor_sent_list['input_ids'][0].shape == tensor_sent_list['input_ids'][1].shape "]},{"cell_type":"code","execution_count":27,"metadata":{"id":"tR22xs-xf1QH"},"outputs":[],"source":["def data_iterator(df, input_column, target_column, batch_size):\n","  \"\"\"\n","  데이터 프레임을 셔플한 후 \n","  데이터 프레임의 input_column을 batch_size만큼 잘라 토크나이즈 + 텐서화하고, target_column을 batch_size만큼 잘라 텐서화 하여\n","  (input, output) 튜플 형태의 이터레이터를 생성\n","  \"\"\"\n","\n","  global tokenizer_bert\n","\n","  # 1. 데이터 프레임 셔플, Freeze/UnFreeze 비교 위해서 random_state 42로 설정.\n","  #    pandas의 sample 함수 사용\n","  df = df.sample(frac=1, random_state = 42).reset_index(drop = True)\n","\n","  # 2. 이터레이터 생성\n","  for idx in range(0, df.shape[0], batch_size):\n","    batch_df = df.iloc[idx:idx+batch_size]\n","    \n","    tensorized_input = tokenizer_bert(\n","      list(batch_df[input_column].values), \n","      add_special_tokens= True,\n","      return_tensors = 'pt',\n","      padding = 'longest'\n","      ) # df의 text를 토크나이징 + token id로 변환 + 텐서화 (df의 input_column 사용)\n","    \n","    tensorized_target = torch.tensor(list(batch_df[target_column].values)) # target(label)을 텐서화 (df의 target_column 사용)\n","    \n","    yield tensorized_input, tensorized_target # 튜플 형태로 yield"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"zTlAV0hqILmc"},"outputs":[],"source":["batch_size=32\n","train_iterator = data_iterator(sample_df, 'document', 'label', batch_size)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"P9VNAMchf1QI"},"outputs":[{"data":{"text/plain":["({'input_ids': tensor([[    2,  8357,  2267,  ...,     0,     0,     0],\n","         [    2,  6108,  2494,  ...,     0,     0,     0],\n","         [    2,  1468, 10809,  ...,     0,     0,     0],\n","         ...,\n","         [    2,  1160,  2259,  ...,     0,     0,     0],\n","         [    2,   732,  2116,  ...,     0,     0,     0],\n","         [    2,    21,  2532,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         ...,\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0],\n","         [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         ...,\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0],\n","         [1, 1, 1,  ..., 0, 0, 0]])},\n"," tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n","         1, 1, 1, 0, 0, 0, 1, 1]))"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["next(train_iterator)"]},{"cell_type":"markdown","metadata":{"id":"Cqnp2Q6ZLnJ2"},"source":["## Advanced"]},{"cell_type":"markdown","metadata":{"id":"cQVTqAUxLnJ2"},"source":["### `data_iterator` 함수로 생성한 이터레이터를 for loop 돌면서 배치 단위의 데이터를 모델에 학습하는 `train()` 함수 구현\n","- 함수 정의\n","  - 입력 매개변수\n","    - `model` : BERT + 1 hidden layer classifier 모델\n","    - `data_iterator` : train data iterator\n","- Reference\n","  - [Loss: CrossEntropyLoss official document](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n","  - [Optimizer: AdamW official document](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"-sE7xjYcRD1p"},"outputs":[],"source":["from torch.optim import AdamW\n","from torch.nn import CrossEntropyLoss\n","from numpy.core.fromnumeric import nonzero"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"7Er1qKtsf1QJ"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["# 모델 클래스 정의\n","model = CustomClassifier(hidden_size=768, n_label=2)\n","\n","batch_size = 32\n","\n","# 데이터 이터레이터 정의 \n","train_iterator = data_iterator(sample_df, 'document', 'label', batch_size)\n","\n","# 로스 및 옵티마이저\n","loss_fct = CrossEntropyLoss()\n","optimizer = AdamW(\n","    model.parameters(),\n","    lr=2e-5,\n","    eps=1e-8\n",")\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"ZvY5rxDKHQAp"},"outputs":[],"source":["def train(model, data_iterator):\n","\n","  global loss_fct # 위에서 정의한 loss 함수\n","\n","  # 배치 단위 평균 loss와 총 평균 loss 계산하기위해 변수 생성\n","  total_loss, batch_loss, batch_count = 0,0,0\n","  \n","  # model을 train 모드로 설정 & device 할당\n","  model.train()\n","  model.to(device)\n","  \n","  # data iterator를 돌면서 배치 단위로 학습\n","  for step, batch in enumerate(data_iterator):\n","    batch_count += 1\n","    \n","    # tensor 연산 전, 각 tensor에 device 할당\n","    batch = tuple(item.to(device) for item in batch)\n","    \n","    batch_input, batch_label = batch # batch가 원래 (text, target) 형태이므로\n","    \n","    # batch마다 모델이 갖고 있는 기존 gradient를 초기화\n","    model.zero_grad()\n","    \n","    # forward\n","    logits = model.forward(\n","      input_ids = batch_input['input_ids'],\n","      attention_mask = batch_input['attention_mask'],\n","      token_type_ids = batch_input['token_type_ids']   \n","    )\n","    \n","    # loss\n","    loss = loss_fct(logits, batch_label) #logits과 label을 비교해서 loss 계산!\n","    batch_loss += loss.item()\n","    total_loss += loss.item()\n","    \n","    # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n","    loss.backward()\n","    \n","    # optimizer 업데이트\n","    optimizer.step()\n","      \n","    # 배치 10개씩 처리할 때마다 평균 loss를 출력\n","    if (step % 10 == 0 and step != 0):\n","      print(f\"Step : {step}, Avg Loss : {batch_loss / batch_count:.4f}\")\n","      \n","      # 변수 초기화 \n","      batch_loss, batch_count = 0,0\n","  \n","  print(f\"Mean Loss : {total_loss/(step+1):.4f}\")\n","  print(\"Train Finished\")"]},{"cell_type":"markdown","metadata":{"id":"OEYo8z9RLnJ2"},"source":["### 지금까지 구현한 함수와 클래스를 모두 불러와 `train()` 함수를 실행하자\n","- fine-tuning 모델 클래스 (`CustomClassifier`)\n","    - hidden_size = 768\n","    - n_label = 2\n","- 데이터 이터레이터 함수 (`data_iterator`)\n","    - batch_size = 32\n","- loss \n","    - `CrossEntropyLoss()`\n","- optimizer\n","    - optimizer는 loss(오차)를 상쇄하기 위해 파라미터를 업데이트 하는 과정\n","    - `optimizer.step()` 시 파라미터가 업데이트 됨 \n","    - lr = 2e-5\n","- Reference\n","  - [Optimizer 종류 설명 한국어 블로그 ](https://ganghee-lee.tistory.com/24)\n","    "]},{"cell_type":"code","execution_count":33,"metadata":{"id":"uCODIxCfFEDP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Step : 10, Avg Loss : 0.6592\n","Step : 20, Avg Loss : 0.6132\n","Step : 30, Avg Loss : 0.5666\n","Mean Loss : 0.6099\n","Train Finished\n"]}],"source":["# 학습 시작\n","train(model, train_iterator)"]},{"cell_type":"markdown","metadata":{"id":"37UbAkh7LnJ3"},"source":["## fine-tuning 2가지 방법론 비교\n","- pre-trained BERT 모델 파라미터를 **freeze**한 채 학습하라\n","    - BERT의 파라미터의 `requires_grad` 값을 `False`로 바꾸면, 학습 시 BERT의 파라미터는 미분이 계산되지도, 업데이트 되지도 않는다. \n","    - 이렇게 특정 모델의 파라미터가 업데이트 하지 못하도록 설정하는 것을 **freeze**라고 한다. \n","    - BERT 파라미터를 freeze시킨 채 학습을 진행해보자. 이럴 경우, 우리가 직접 쌓은 fine-tuning layer의 파라미터만 업데이트 된다. \n","- **unfreeze**와 **freeze** 모델의 성능을 비교해 보자. 어떤 방식이 더 우수한가?\n","\n","    "]},{"cell_type":"code","execution_count":34,"metadata":{"id":"Ld260K3YLnJ3"},"outputs":[],"source":["class CustomClassifierFreezed(nn.Module):\n","\n","  def __init__(self, hidden_size: int, n_label: int):\n","    super(CustomClassifierFreezed, self).__init__()\n","\n","    self.bert = BertModel.from_pretrained(\"klue/bert-base\")\n","    # freeze BERT parameter\n","    # BERT의 파라미터는 고정값으로 두고 BERT 위에 씌운 linear layer의 파라미터만 학습하려고 한다. \n","    # 이 경우, BERT의 파라미터의 'requires_grad' 값을 False로 변경해줘야 학습 시 해당 파라미터의 미분값이 계산되지 않는다.\n","    for param in self.bert.parameters():\n","        param.requires_grad = False\n","\n","    dropout_rate = 0.1\n","    linear_layer_hidden_size = 32\n","    \n","    # classifier 생성\n","    self.classifier = nn.Sequential(\n","      nn.Linear(hidden_size, linear_layer_hidden_size), # input shape : hidden_size, output shape : 32(linear_layer_hidden_size)\n","      nn.ReLU(),\n","      nn.Dropout(),\n","      nn.Linear(linear_layer_hidden_size, n_label) # input shape : 32, output shape : 2(n_label)  \n","      ) # torch.nn에서 제공되는 Sequential, Linear, ReLU, Dropout 함수 활용\n","\n","      \n","  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n","\n","    outputs = self.bert(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        token_type_ids=token_type_ids,\n","    )\n","    \n","    # BERT 모델의 마지막 레이어의 첫번재 토큰을 인덱싱\n","    cls_token_last_hidden_states = outputs['pooler_output'] # 마지막 layer의 첫 번째 토큰 (\"[CLS]\") 벡터를 가져오기, shape = (1, hidden_size)\n","    logits = self.classifier(cls_token_last_hidden_states)\n","\n","    return logits"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"2ClEEHB6F6LW"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["# freeze 모델\n","# model을 제외한 설정값은 \b위에서 실행한 unfreeze 모델과 동일\n","model = CustomClassifierFreezed(hidden_size = 768, n_label = 2)\n","\n","# 데이터 이터레이터\n","batch_size = 32\n","train_iterator = data_iterator(sample_df, 'document', 'label', batch_size)\n","\n","# 로스 및 옵티마이저\n","loss_fct = CrossEntropyLoss()\n","optimizer = AdamW(\n","    model.parameters(),\n","    lr = 2e-5,\n","    eps = 1e-8\n",")"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"hqcPcWZeLnJ3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Step : 10, Avg Loss : 0.6931\n","Step : 20, Avg Loss : 0.6895\n","Step : 30, Avg Loss : 0.6848\n","Mean Loss : 0.6885\n","Train Finished\n"]}],"source":["# 학습 시작\n","train(model, train_iterator)"]},{"cell_type":"markdown","metadata":{},"source":["___"]},{"cell_type":"markdown","metadata":{},"source":["#### UnFreeze\n","Step : 10, Avg Loss : 0.6592   \n","Step : 20, Avg Loss : 0.6132   \n","Step : 30, Avg Loss : 0.5666   \n","Mean Loss : 0.6099 (Time : 8.9s)   \n","\n","#### Freeze\n","Step : 10, Avg Loss : 0.6931   \n","Step : 20, Avg Loss : 0.6895   \n","Step : 30, Avg Loss : 0.6848   \n","Mean Loss : 0.6885 (Time : 2.2s)   \n","\n","\n","정확도 측면에선 UnFreeze 방법을 사용할 때 더 낮은 Loss를 기록했다.   \n","하지만 **매우 적은 수**의 **Train data만** 가지고 계산된 Loss이기 때문에 과적합 우려가 있어 어느 방법이 더 우수한지 당장은 알 수 없다.   \n","추후에 Validation/Test data로 확인해보면 명확해질 것이다. \n","\n","\n","시간 측면에선 Freeze 방법을 사용했을 때(2.2s) UnFreeze 모델 대비(8.9s) 빠른 속도를 보여줬다.   \n","update를 할 grad가 적기 때문에 당연한 결과다.   \n","\n","실제 사전 훈련된 모델을 적용할 땐 정확도와 시간을 모두 고려하여 목적에 맞는 방법을 사용하는 것이 중요하다.\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Week2_2_assignment.ipynb","provenance":[]},"interpreter":{"hash":"70a8288ab958a215e8d75c20ae32722b3e32ac0386630f0daa1847295c58910f"},"kernelspec":{"display_name":"torch","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
