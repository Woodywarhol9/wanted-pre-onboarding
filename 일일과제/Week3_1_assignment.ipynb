{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugGR7pnI4WSe"
      },
      "source": [
        "# Week3_1 Assignment\n",
        "\n",
        "## [BASIC](#Basic) \n",
        "- 토크나이징이 완료된 위키 백과 코퍼스를 다운받고 **단어 사전을 구축하는 함수를 구현**할 수 있다.\n",
        "- `Skip-Gram` 방식의 학습 데이터 셋을 생성하는 **Dataset과 Dataloader 클래스를 구현**할 수 있다.\n",
        "- **Negative Sampling** 함수를 구현할 수 있다. \n",
        "\n",
        "\n",
        "## [CHALLENGE](#Challenge)\n",
        "- Skip-Gram을 학습 과정 튜토리얼을 따라하며, **Skip-Gram을 학습하는 클래스를 구현**할 수 있다. \n",
        "\n",
        "\n",
        "## [ADVANCED](#Advanced)\n",
        "- Skip-Gram 방식으로 word embedding을 학습하는 **Word2Vec 클래스를 구현**하고 실제로 학습할 수 있다.\n",
        "- 학습이 완료된 word embedding을 불러와 **Gensim 패키지를 사용해 유사한 단어**를 뽑을 수 있다. \n",
        "\n",
        "### Reference\n",
        "- [Skip-Gram negative sampling 한국어 튜토리얼](https://wikidocs.net/69141)\n",
        "    - (참고) 위 튜토리얼에서는 target word와 context word 페어의 레이블은 1로, target word와 negative sample word 페어의 레이블은 0이 되도록 학습 데이터를 구현해 binary classification을 구현한다. 하지만 우리는 word2vec 논문 방식을 그대로 따르기 위해 label을 생성하지 않고 대신 loss 함수를 변행해서 binary classification을 학습할 것이다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:29:36.641276Z",
          "start_time": "2022-02-19T14:29:36.638642Z"
        },
        "id": "HlEy3xfY4WSh"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:50:41.644583Z",
          "start_time": "2022-02-19T12:50:41.642937Z"
        },
        "id": "cBrr7-gt4jnf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/wjs/anaconda3/lib/python3.9/site-packages (4.16.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: requests in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (3.3.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/wjs/anaconda3/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wjs/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/wjs/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/wjs/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/wjs/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/wjs/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/wjs/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /home/wjs/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /home/wjs/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /home/wjs/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:26:59.276355Z",
          "start_time": "2022-02-19T14:26:58.411434Z"
        },
        "id": "6mC9lhsJ4WSh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-03-08 21:02:09.603733: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:30:05.586472Z",
          "start_time": "2022-02-19T14:30:05.583611Z"
        },
        "id": "17g7UZ5g4WSi"
      },
      "outputs": [],
      "source": [
        "# seed\n",
        "seed = 7777\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:30:06.721039Z",
          "start_time": "2022-02-19T14:30:06.717559Z"
        },
        "id": "v3UlC7Jn4WSi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Graphics Device\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# device type\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8sfv5KY4WSk"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHs8_LU04WSj"
      },
      "source": [
        "### 토크나이징이 완료된 위키 백과 코퍼스 다운로드 및 불용어 사전 크롤링\n",
        "- 나의 구글 드라이브에 데이터를 다운받아 영구적으로 사용할 수 있도록 하자. \n",
        "    - [데이터 다운로드 출처](https://ratsgo.github.io/embedding/downloaddata.html)\n",
        "- 다운받은 데이터는 토크나이징이 완료된 상태이지만 불용어를 포함하고 있다. 따라서 향후 불용어를 제거하기 위해 불용어 사전을 크롤링하자. \n",
        "    - [불용어 사전 출처](https://www.ranks.nl/stopwords/korean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KYiz1fdNsAqp"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z2WZ0P4wsAqp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/wjs/workspace/pre_onboarding/data\n"
          ]
        }
      ],
      "source": [
        "cd data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:11.886643Z",
          "start_time": "2022-02-19T14:27:11.884858Z"
        },
        "id": "4QPBJ6UZ4WSj"
      },
      "outputs": [],
      "source": [
        "# 데이터 다운로드\n",
        "#!pip install gdown\n",
        "#!gdown https://drive.google.com/u/0/uc?id=1Ybp_DmzNEpsBrUKZ1-NoPDzCMO39f-fx\n",
        "#!unzip tokenized.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:15.633947Z",
          "start_time": "2022-02-19T14:27:13.829982Z"
        },
        "id": "cTHCHmO24WSj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wjs/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.ranks.nl'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Korean stop words: 677\n"
          ]
        }
      ],
      "source": [
        "# 한국어 불용어 리스트 크롤링\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.ranks.nl/stopwords/korean\"\n",
        "response = requests.get(url, verify = False)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text,'html.parser')\n",
        "    content = soup.select_one('#article178ebefbfb1b165454ec9f168f545239 > div.panel-body > table > tbody > tr')\n",
        "    stop_words=[]\n",
        "    for x in content.strings:\n",
        "        x=x.strip()\n",
        "        if x:\n",
        "            stop_words.append(x)\n",
        "    print(f\"# Korean stop words: {len(stop_words)}\")\n",
        "else:\n",
        "    print(response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:15.642775Z",
          "start_time": "2022-02-19T14:27:15.635333Z"
        },
        "id": "3d0IqhDF4WSk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['아',\n",
              " '휴',\n",
              " '아이구',\n",
              " '아이쿠',\n",
              " '아이고',\n",
              " '어',\n",
              " '나',\n",
              " '우리',\n",
              " '저희',\n",
              " '따라',\n",
              " '의해',\n",
              " '을',\n",
              " '를',\n",
              " '에',\n",
              " '의',\n",
              " '가',\n",
              " '으로',\n",
              " '로',\n",
              " '에게',\n",
              " '뿐이다',\n",
              " '의거하여',\n",
              " '근거하여',\n",
              " '입각하여',\n",
              " '기준으로',\n",
              " '예하면',\n",
              " '예를 들면',\n",
              " '예를 들자면',\n",
              " '저',\n",
              " '소인',\n",
              " '소생',\n",
              " '저희',\n",
              " '지말고',\n",
              " '하지마',\n",
              " '하지마라',\n",
              " '다른',\n",
              " '물론',\n",
              " '또한',\n",
              " '그리고',\n",
              " '비길수 없다',\n",
              " '해서는 안된다',\n",
              " '뿐만 아니라',\n",
              " '만이 아니다',\n",
              " '만은 아니다',\n",
              " '막론하고',\n",
              " '관계없이',\n",
              " '그치지 않다',\n",
              " '그러나',\n",
              " '그런데',\n",
              " '하지만',\n",
              " '든간에',\n",
              " '논하지 않다',\n",
              " '따지지 않다',\n",
              " '설사',\n",
              " '비록',\n",
              " '더라도',\n",
              " '아니면',\n",
              " '만 못하다',\n",
              " '하는 편이 낫다',\n",
              " '불문하고',\n",
              " '향하여',\n",
              " '향해서',\n",
              " '향하다',\n",
              " '쪽으로',\n",
              " '틈타',\n",
              " '이용하여',\n",
              " '타다',\n",
              " '오르다',\n",
              " '제외하고',\n",
              " '이 외에',\n",
              " '이 밖에',\n",
              " '하여야',\n",
              " '비로소',\n",
              " '한다면 몰라도',\n",
              " '외에도',\n",
              " '이곳',\n",
              " '여기',\n",
              " '부터',\n",
              " '기점으로',\n",
              " '따라서',\n",
              " '할 생각이다',\n",
              " '하려고하다',\n",
              " '이리하여',\n",
              " '그리하여',\n",
              " '그렇게 함으로써',\n",
              " '하지만',\n",
              " '일때',\n",
              " '할때',\n",
              " '앞에서',\n",
              " '중에서',\n",
              " '보는데서',\n",
              " '으로써',\n",
              " '로써',\n",
              " '까지',\n",
              " '해야한다',\n",
              " '일것이다',\n",
              " '반드시',\n",
              " '할줄알다',\n",
              " '할수있다',\n",
              " '할수있어',\n",
              " '임에 틀림없다',\n",
              " '한다면',\n",
              " '등',\n",
              " '등등',\n",
              " '제',\n",
              " '겨우',\n",
              " '단지',\n",
              " '다만',\n",
              " '할뿐',\n",
              " '딩동',\n",
              " '댕그',\n",
              " '대해서',\n",
              " '대하여',\n",
              " '대하면',\n",
              " '훨씬',\n",
              " '얼마나',\n",
              " '얼마만큼',\n",
              " '얼마큼',\n",
              " '남짓',\n",
              " '여',\n",
              " '얼마간',\n",
              " '약간',\n",
              " '다소',\n",
              " '좀',\n",
              " '조금',\n",
              " '다수',\n",
              " '몇',\n",
              " '얼마',\n",
              " '지만',\n",
              " '하물며',\n",
              " '또한',\n",
              " '그러나',\n",
              " '그렇지만',\n",
              " '하지만',\n",
              " '이외에도',\n",
              " '대해 말하자면',\n",
              " '뿐이다',\n",
              " '다음에',\n",
              " '반대로',\n",
              " '반대로 말하자면',\n",
              " '이와 반대로',\n",
              " '바꾸어서 말하면',\n",
              " '바꾸어서 한다면',\n",
              " '만약',\n",
              " '그렇지않으면',\n",
              " '까악',\n",
              " '툭',\n",
              " '딱',\n",
              " '삐걱거리다',\n",
              " '보드득',\n",
              " '비걱거리다',\n",
              " '꽈당',\n",
              " '응당',\n",
              " '해야한다',\n",
              " '에 가서',\n",
              " '각',\n",
              " '각각',\n",
              " '여러분',\n",
              " '각종',\n",
              " '각자',\n",
              " '제각기',\n",
              " '하도록하다',\n",
              " '와',\n",
              " '과',\n",
              " '그러므로',\n",
              " '그래서',\n",
              " '고로',\n",
              " '한 까닭에',\n",
              " '하기 때문에',\n",
              " '거니와',\n",
              " '이지만',\n",
              " '대하여',\n",
              " '관하여',\n",
              " '관한',\n",
              " '과연',\n",
              " '실로',\n",
              " '아니나다를가',\n",
              " '생각한대로',\n",
              " '진짜로',\n",
              " '한적이있다',\n",
              " '하곤하였다',\n",
              " '하',\n",
              " '하하',\n",
              " '허허',\n",
              " '아하',\n",
              " '거바',\n",
              " '와',\n",
              " '오',\n",
              " '왜',\n",
              " '어째서',\n",
              " '무엇때문에',\n",
              " '어찌',\n",
              " '하겠는가',\n",
              " '무슨',\n",
              " '어디',\n",
              " '어느곳',\n",
              " '더군다나',\n",
              " '하물며',\n",
              " '더욱이는',\n",
              " '어느때',\n",
              " '언제',\n",
              " '야',\n",
              " '이봐',\n",
              " '어이',\n",
              " '여보시오',\n",
              " '흐흐',\n",
              " '흥',\n",
              " '휴',\n",
              " '헉헉',\n",
              " '헐떡헐떡',\n",
              " '영차',\n",
              " '여차',\n",
              " '어기여차',\n",
              " '끙끙',\n",
              " '아야',\n",
              " '앗',\n",
              " '아야',\n",
              " '콸콸',\n",
              " '졸졸',\n",
              " '좍좍',\n",
              " '뚝뚝',\n",
              " '주룩주룩',\n",
              " '솨',\n",
              " '우르르',\n",
              " '그래도',\n",
              " '또',\n",
              " '그리고',\n",
              " '바꾸어말하면',\n",
              " '바꾸어말하자면',\n",
              " '혹은',\n",
              " '혹시',\n",
              " '답다',\n",
              " '및',\n",
              " '그에 따르는',\n",
              " '때가 되어',\n",
              " '즉',\n",
              " '지든지',\n",
              " '설령',\n",
              " '가령',\n",
              " '하더라도',\n",
              " '할지라도',\n",
              " '일지라도',\n",
              " '지든지',\n",
              " '몇',\n",
              " '거의',\n",
              " '하마터면',\n",
              " '인젠',\n",
              " '이젠',\n",
              " '된바에야',\n",
              " '된이상',\n",
              " '만큼',\n",
              " '어찌됏든',\n",
              " '그위에',\n",
              " '게다가',\n",
              " '점에서 보아',\n",
              " '비추어 보아',\n",
              " '고려하면',\n",
              " '하게될것이다',\n",
              " '일것이다',\n",
              " '비교적',\n",
              " '좀',\n",
              " '보다더',\n",
              " '비하면',\n",
              " '시키다',\n",
              " '하게하다',\n",
              " '할만하다',\n",
              " '의해서',\n",
              " '연이서',\n",
              " '이어서',\n",
              " '잇따라',\n",
              " '뒤따라',\n",
              " '뒤이어',\n",
              " '결국',\n",
              " '의지하여',\n",
              " '기대여',\n",
              " '통하여',\n",
              " '자마자',\n",
              " '더욱더',\n",
              " '불구하고',\n",
              " '얼마든지',\n",
              " '마음대로',\n",
              " '주저하지 않고',\n",
              " '곧',\n",
              " '즉시',\n",
              " '바로',\n",
              " '당장',\n",
              " '하자마자',\n",
              " '밖에 안된다',\n",
              " '하면된다',\n",
              " '그래',\n",
              " '그렇지',\n",
              " '요컨대',\n",
              " '다시 말하자면',\n",
              " '바꿔 말하면',\n",
              " '즉',\n",
              " '구체적으로',\n",
              " '말하자면',\n",
              " '시작하여',\n",
              " '시초에',\n",
              " '이상',\n",
              " '허',\n",
              " '헉',\n",
              " '허걱',\n",
              " '바와같이',\n",
              " '해도좋다',\n",
              " '해도된다',\n",
              " '게다가',\n",
              " '더구나',\n",
              " '하물며',\n",
              " '와르르',\n",
              " '팍',\n",
              " '퍽',\n",
              " '펄렁',\n",
              " '동안',\n",
              " '이래',\n",
              " '하고있었다',\n",
              " '이었다',\n",
              " '에서',\n",
              " '로부터',\n",
              " '까지',\n",
              " '예하면',\n",
              " '했어요',\n",
              " '해요',\n",
              " '함께',\n",
              " '같이',\n",
              " '더불어',\n",
              " '마저',\n",
              " '마저도',\n",
              " '양자',\n",
              " '모두',\n",
              " '습니다',\n",
              " '가까스로',\n",
              " '하려고하다',\n",
              " '즈음하여',\n",
              " '다른',\n",
              " '다른 방면으로',\n",
              " '해봐요',\n",
              " '습니까',\n",
              " '했어요',\n",
              " '말할것도 없고',\n",
              " '무릎쓰고',\n",
              " '개의치않고',\n",
              " '하는것만 못하다',\n",
              " '하는것이 낫다',\n",
              " '매',\n",
              " '매번',\n",
              " '들',\n",
              " '모',\n",
              " '어느것',\n",
              " '어느',\n",
              " '로써',\n",
              " '갖고말하자면',\n",
              " '어디',\n",
              " '어느쪽',\n",
              " '어느것',\n",
              " '어느해',\n",
              " '어느 년도',\n",
              " '라 해도',\n",
              " '언젠가',\n",
              " '어떤것',\n",
              " '어느것',\n",
              " '저기',\n",
              " '저쪽',\n",
              " '저것',\n",
              " '그때',\n",
              " '그럼',\n",
              " '그러면',\n",
              " '요만한걸',\n",
              " '그래',\n",
              " '그때',\n",
              " '저것만큼',\n",
              " '그저',\n",
              " '이르기까지',\n",
              " '할 줄 안다',\n",
              " '할 힘이 있다',\n",
              " '너',\n",
              " '너희',\n",
              " '당신',\n",
              " '어찌',\n",
              " '설마',\n",
              " '차라리',\n",
              " '할지언정',\n",
              " '할지라도',\n",
              " '할망정',\n",
              " '할지언정',\n",
              " '구토하다',\n",
              " '게우다',\n",
              " '토하다',\n",
              " '메쓰겁다',\n",
              " '옆사람',\n",
              " '퉤',\n",
              " '쳇',\n",
              " '의거하여',\n",
              " '근거하여',\n",
              " '의해',\n",
              " '따라',\n",
              " '힘입어',\n",
              " '그',\n",
              " '다음',\n",
              " '버금',\n",
              " '두번째로',\n",
              " '기타',\n",
              " '첫번째로',\n",
              " '나머지는',\n",
              " '그중에서',\n",
              " '견지에서',\n",
              " '형식으로 쓰여',\n",
              " '입장에서',\n",
              " '위해서',\n",
              " '단지',\n",
              " '의해되다',\n",
              " '하도록시키다',\n",
              " '뿐만아니라',\n",
              " '반대로',\n",
              " '전후',\n",
              " '전자',\n",
              " '앞의것',\n",
              " '잠시',\n",
              " '잠깐',\n",
              " '하면서',\n",
              " '그렇지만',\n",
              " '다음에',\n",
              " '그러한즉',\n",
              " '그런즉',\n",
              " '남들',\n",
              " '아무거나',\n",
              " '어찌하든지',\n",
              " '같다',\n",
              " '비슷하다',\n",
              " '예컨대',\n",
              " '이럴정도로',\n",
              " '어떻게',\n",
              " '만약',\n",
              " '만일',\n",
              " '위에서 서술한바와같이',\n",
              " '인 듯하다',\n",
              " '하지 않는다면',\n",
              " '만약에',\n",
              " '무엇',\n",
              " '무슨',\n",
              " '어느',\n",
              " '어떤',\n",
              " '아래윗',\n",
              " '조차',\n",
              " '한데',\n",
              " '그럼에도 불구하고',\n",
              " '여전히',\n",
              " '심지어',\n",
              " '까지도',\n",
              " '조차도',\n",
              " '하지 않도록',\n",
              " '않기 위하여',\n",
              " '때',\n",
              " '시각',\n",
              " '무렵',\n",
              " '시간',\n",
              " '동안',\n",
              " '어때',\n",
              " '어떠한',\n",
              " '하여금',\n",
              " '네',\n",
              " '예',\n",
              " '우선',\n",
              " '누구',\n",
              " '누가 알겠는가',\n",
              " '아무도',\n",
              " '줄은모른다',\n",
              " '줄은 몰랏다',\n",
              " '하는 김에',\n",
              " '겸사겸사',\n",
              " '하는바',\n",
              " '그런 까닭에',\n",
              " '한 이유는',\n",
              " '그러니',\n",
              " '그러니까',\n",
              " '때문에',\n",
              " '그',\n",
              " '너희',\n",
              " '그들',\n",
              " '너희들',\n",
              " '타인',\n",
              " '것',\n",
              " '것들',\n",
              " '너',\n",
              " '위하여',\n",
              " '공동으로',\n",
              " '동시에',\n",
              " '하기 위하여',\n",
              " '어찌하여',\n",
              " '무엇때문에',\n",
              " '붕붕',\n",
              " '윙윙',\n",
              " '나',\n",
              " '우리',\n",
              " '엉엉',\n",
              " '휘익',\n",
              " '윙윙',\n",
              " '오호',\n",
              " '아하',\n",
              " '어쨋든',\n",
              " '만 못하다',\n",
              " '하기보다는',\n",
              " '차라리',\n",
              " '하는 편이 낫다',\n",
              " '흐흐',\n",
              " '놀라다',\n",
              " '상대적으로 말하자면',\n",
              " '마치',\n",
              " '아니라면',\n",
              " '쉿',\n",
              " '그렇지 않으면',\n",
              " '그렇지 않다면',\n",
              " '안 그러면',\n",
              " '아니었다면',\n",
              " '하든지',\n",
              " '아니면',\n",
              " '이라면',\n",
              " '좋아',\n",
              " '알았어',\n",
              " '하는것도',\n",
              " '그만이다',\n",
              " '어쩔수 없다',\n",
              " '하나',\n",
              " '일',\n",
              " '일반적으로',\n",
              " '일단',\n",
              " '한켠으로는',\n",
              " '오자마자',\n",
              " '이렇게되면',\n",
              " '이와같다면',\n",
              " '전부',\n",
              " '한마디',\n",
              " '한항목',\n",
              " '근거로',\n",
              " '하기에',\n",
              " '아울러',\n",
              " '하지 않도록',\n",
              " '않기 위해서',\n",
              " '이르기까지',\n",
              " '이 되다',\n",
              " '로 인하여',\n",
              " '까닭으로',\n",
              " '이유만으로',\n",
              " '이로 인하여',\n",
              " '그래서',\n",
              " '이 때문에',\n",
              " '그러므로',\n",
              " '그런 까닭에',\n",
              " '알 수 있다',\n",
              " '결론을 낼 수 있다',\n",
              " '으로 인하여',\n",
              " '있다',\n",
              " '어떤것',\n",
              " '관계가 있다',\n",
              " '관련이 있다',\n",
              " '연관되다',\n",
              " '어떤것들',\n",
              " '에 대해',\n",
              " '이리하여',\n",
              " '그리하여',\n",
              " '여부',\n",
              " '하기보다는',\n",
              " '하느니',\n",
              " '하면 할수록',\n",
              " '운운',\n",
              " '이러이러하다',\n",
              " '하구나',\n",
              " '하도다',\n",
              " '다시말하면',\n",
              " '다음으로',\n",
              " '에 있다',\n",
              " '에 달려 있다',\n",
              " '우리',\n",
              " '우리들',\n",
              " '오히려',\n",
              " '하기는한데',\n",
              " '어떻게',\n",
              " '어떻해',\n",
              " '어찌됏어',\n",
              " '어때',\n",
              " '어째서',\n",
              " '본대로',\n",
              " '자',\n",
              " '이',\n",
              " '이쪽',\n",
              " '여기',\n",
              " '이것',\n",
              " '이번',\n",
              " '이렇게말하자면',\n",
              " '이런',\n",
              " '이러한',\n",
              " '이와 같은',\n",
              " '요만큼',\n",
              " '요만한 것',\n",
              " '얼마 안 되는 것',\n",
              " '이만큼',\n",
              " '이 정도의',\n",
              " '이렇게 많은 것',\n",
              " '이와 같다',\n",
              " '이때',\n",
              " '이렇구나',\n",
              " '것과 같이',\n",
              " '끼익',\n",
              " '삐걱',\n",
              " '따위',\n",
              " '와 같은 사람들',\n",
              " '부류의 사람들',\n",
              " '왜냐하면',\n",
              " '중의하나',\n",
              " '오직',\n",
              " '오로지',\n",
              " '에 한하다',\n",
              " '하기만 하면',\n",
              " '도착하다',\n",
              " '까지 미치다',\n",
              " '도달하다',\n",
              " '정도에 이르다',\n",
              " '할 지경이다',\n",
              " '결과에 이르다',\n",
              " '관해서는',\n",
              " '여러분',\n",
              " '하고 있다',\n",
              " '한 후',\n",
              " '혼자',\n",
              " '자기',\n",
              " '자기집',\n",
              " '자신',\n",
              " '우에 종합한것과같이',\n",
              " '총적으로 보면',\n",
              " '총적으로 말하면',\n",
              " '총적으로',\n",
              " '대로 하다',\n",
              " '으로서',\n",
              " '참',\n",
              " '그만이다',\n",
              " '할 따름이다',\n",
              " '쿵',\n",
              " '탕탕',\n",
              " '쾅쾅',\n",
              " '둥둥',\n",
              " '봐',\n",
              " '봐라',\n",
              " '아이야',\n",
              " '아니',\n",
              " '와아',\n",
              " '응',\n",
              " '아이',\n",
              " '참나',\n",
              " '년',\n",
              " '월',\n",
              " '일',\n",
              " '령',\n",
              " '영',\n",
              " '일',\n",
              " '이',\n",
              " '삼',\n",
              " '사',\n",
              " '오',\n",
              " '육',\n",
              " '륙',\n",
              " '칠',\n",
              " '팔',\n",
              " '구',\n",
              " '이천육',\n",
              " '이천칠',\n",
              " '이천팔',\n",
              " '이천구',\n",
              " '하나',\n",
              " '둘',\n",
              " '셋',\n",
              " '넷',\n",
              " '다섯',\n",
              " '여섯',\n",
              " '일곱',\n",
              " '여덟',\n",
              " '아홉',\n",
              " '령',\n",
              " '영']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t76Q1pQ4WSk"
      },
      "source": [
        "### 단어 사전 구축 함수 구현 \n",
        "- 문서 리스트를 입력 받아 사전을 생성하는 `make_vocab()` 함수를 구현하라.\n",
        "- 함수 정의\n",
        "    - 입력 매개변수\n",
        "        - docs : 문서 리스트\n",
        "        - min_count : 최소 단어 등장 빈도수 (단어 빈도가 `min_count` 미만인 단어는 사전에 포함하지 않음)\n",
        "    - 조건\n",
        "        - 문서 길이 제한\n",
        "            - 단어 개수가 3개 이하인 문서는 처리하지 않음. (skip)\n",
        "        - 사전에 포함되는 단어 빈도수 제한\n",
        "            - 단어가 빈도가 `min_count` 미만은 단어는 사전에 포함하지 않음.\n",
        "        - 불용어 제거 \n",
        "            - 불용어 리스트에 포함된 단어는 제거 \n",
        "    - 반환값 \n",
        "        - word2count : 단어별 빈도 사전 (key: 단어, value: 등장 횟수)\n",
        "        - wid2word : 단어별 인덱스(wid) 사전 (key: 단어 인덱스(int), value: 단어)\n",
        "        - word2wid : 인덱스(wid)별 단어 사전 (key: 단어, value: 단어 인덱스(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:01.637431Z",
          "start_time": "2022-02-19T14:32:56.730711Z"
        },
        "id": "xkjqztIA4WSl"
      },
      "outputs": [],
      "source": [
        "# 코퍼스 로드\n",
        "#with문으로 close()없이 read\n",
        "with open (\"tokenized/wiki_ko_mecab.txt\", 'r') as file:\n",
        "    docs = file.readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 총 311,237개의 문서"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:03.423002Z",
          "start_time": "2022-02-19T14:33:03.419818Z"
        },
        "id": "WAKB6bbt4WSl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# wiki documents: 311,237\n"
          ]
        }
      ],
      "source": [
        "print(f\"# wiki documents: {len(docs):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:04.016885Z",
          "start_time": "2022-02-19T14:33:03.962269Z"
        },
        "id": "-OI1MCXv4WSl"
      },
      "outputs": [],
      "source": [
        "# 문서 개수를 500개로 줄임\n",
        "random.seed(seed) #seed 7777\n",
        "docs=random.sample(docs,500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'남모 공주 ( 南 毛 公主 ) 는 신라 의 공주 , 왕족 으로 법흥왕 과 보과 공주 부여 씨 의 딸 이 며 백제 동성왕 의 외손녀 였 다 . 경쟁자 인 준정 과 함께 신라 의 초대 여성 원화 ( 화랑 ) 였 다 . 그 가 준정 에게 암살 당한 것 을 계기 로 화랑 은 여성 이 아닌 남성 미소년 으로 선발 하 게 되 었 다 . 신라 진흥왕 에게 는 사촌 누나 이 자 이모 가 된다 . 신라 의 청소년 조직 이 었 던 화랑도 는 처음 에 는 남모 , 준정 두 미녀 를 뽑 아 이 를 ` 원화 라 했으며 이 들 주위 에 는 3 0 0 여 명 의 무리 를 따르 게 하 였 다 . 그러나 준정 과 남모 는 서로 최고 가 되 고자 시기 하 였 다 . 준정 은 박영실 을 섬겼 는데 , 지소태후 는 자신 의 두 번 째 남편 이 기 도 한 그 를 싫어해서 준정 의 원화 를 없애 고 낭도 가 부족 한 남모 에게 위화랑 의 낭도 를 더 해 주 었 다 . 그 뒤 남모 는 준정 의 초대 로 그 의 집 에 갔 다가 억지로 권하 는 술 을 받아마시 고 취한 뒤 준정 에 의해 강물 에 던져져 살해 되 었 다 . 이 일 이 발각 돼 준 정도 사형 에 처해지 고 나라 에서 는 귀족 출신 의 잘 생기 고 품행 이 곧 은 남자 를 뽑 아 곱 게 단장 한 후 이 를 화랑 이 라 칭하 고 받들 게 하 였 다 . . 부왕 신라 제 2 3 대 국왕 법흥왕 모후 보과 공주 부여 씨 ( 宝 果 公主 扶餘 氏 ) * 공주 남모 공주 외조부 백제 제 2 4 대 국왕 동성왕 외조모 신라 이찬 비지 의 딸 《 화랑전사 마루 》( KBS , 2 0 0 6 년 , 배우 : 박효빈 ) 신라 법흥왕 백제 동성왕 준정 화랑 분류 : 5 7 6 년 죽음 분류 : 신라 의 왕녀 분류 : 신라 의 왕족 분류 : 화랑 분류 : 암살 된 사람 분류 : 독살 된 사람 분류 : 법흥왕\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mP5wGu9YwDUw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# wiki documents: 500\n"
          ]
        }
      ],
      "source": [
        "print(f\"# wiki documents: {len(docs):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:26.392627Z",
          "start_time": "2022-02-19T14:33:26.382358Z"
        },
        "id": "aJaEAVm9sAqv"
      },
      "outputs": [],
      "source": [
        "# 문서 내 숫자, 영어 대소문자, 특수문자를 제거 (re package 사용)\n",
        "# 한글만 남겨서 처리\n",
        "hangul = re.compile('[^ ㄱ-ㅣ가-힣+]')\n",
        "docs = [hangul.sub(\"\", i) for i in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sytiSICawMk5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Check : 남모 공주      는 신라 의 공주  왕족 으로 법흥왕 과 보과 공주 부여 씨 의 딸 이 며 백제 동성왕 의 외손녀 였 다  경쟁자 인 준정 과 함께 신라 의 초대 여성 원화  화랑  였 다  그 가 준정 에게 암살 당한 것 을 계기 로 화랑 은 여성 이 아닌 남성 미소년 으로 선발 하 게 되 었 다  신라 진흥왕 에게 는 사촌 누나 이 자 이모 가 된다  신라 의 청소년 조직 이 었 던 화랑도 는 처음 에 는 남모  준정 두 미녀 를 뽑 아 이 를  원화 라 했으며 이 들 주위 에 는    여 명 의 무리 를 따르 게 하 였 다  그러나 준정 과 남모 는 서로 최고 가 되 고자 시기 하 였 다  준정 은 박영실 을 섬겼 는데  지소태후 는 자신 의 두 번 째 남편 이 기 도 한 그 를 싫어해서 준정 의 원화 를 없애 고 낭도 가 부족 한 남모 에게 위화랑 의 낭도 를 더 해 주 었 다  그 뒤 남모 는 준정 의 초대 로 그 의 집 에 갔 다가 억지로 권하 는 술 을 받아마시 고 취한 뒤 준정 에 의해 강물 에 던져져 살해 되 었 다  이 일 이 발각 돼 준 정도 사형 에 처해지 고 나라 에서 는 귀족 출신 의 잘 생기 고 품행 이 곧 은 남자 를 뽑 아 곱 게 단장 한 후 이 를 화랑 이 라 칭하 고 받들 게 하 였 다   부왕 신라 제   대 국왕 법흥왕 모후 보과 공주 부여 씨         공주 남모 공주 외조부 백제 제   대 국왕 동성왕 외조모 신라 이찬 비지 의 딸  화랑전사 마루        년  배우  박효빈  신라 법흥왕 백제 동성왕 준정 화랑 분류     년 죽음 분류  신라 의 왕녀 분류  신라 의 왕족 분류  화랑 분류  암살 된 사람 분류  독살 된 사람 분류  법흥왕\n"
          ]
        }
      ],
      "source": [
        "print(f\"Check : {docs[0][:1000]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:27.904880Z",
          "start_time": "2022-02-19T14:33:27.899620Z"
        },
        "id": "OAkkQsvO4WSl"
      },
      "outputs": [],
      "source": [
        "def make_vocab(docs:List[str], min_count:int):\n",
        "    \"\"\"\n",
        "    'docs'문서 리스트를 입력 받아 단어 사전을 생성.\n",
        "    \n",
        "    return \n",
        "        - word2count : 단어별 빈도 사전\n",
        "        - wid2word : 단어별 인덱스(wid) 사전 \n",
        "        - word2wid : 인덱스(wid)별 단어 사전\n",
        "    \"\"\"\n",
        "\n",
        "    word2count = dict()\n",
        "    word2id = dict()\n",
        "    id2word = dict()\n",
        "\n",
        "    \n",
        "    for doc in tqdm(docs):\n",
        "        word_list = doc.split()\n",
        "\n",
        "        # 1. 문서 길이 제한\n",
        "        if len(word_list) <= 3:\n",
        "            continue        \n",
        "        # 2. 임시 딕셔너리(_word2count)에 단어별 등장 빈도 기록\n",
        "        for word in word_list:    \n",
        "            if word not in word2count:\n",
        "                word2count[word] = 0\n",
        "            word2count[word] += 1\n",
        "               \n",
        "    # 3. 불용어 제거\n",
        "    for s_word in stop_words:\n",
        "        if s_word in word2count:\n",
        "            word2count.pop(s_word)\n",
        "    # 4. 토큰 최소 빈도를 만족하는 토큰만 사전에 추가\n",
        "    # 최소 빈도 토큰 제외\n",
        "    word2count = {k : v for k,v in word2count.items() if v >= min_count}\n",
        "    # 빈도순으로 정렬\n",
        "    word2count = dict(sorted(word2count.items(), key = lambda x : x[1], reverse = True))\n",
        "\n",
        "    # word2id, id2word 생성\n",
        "    word2id = dict(zip(word2count.keys(), range(len(word2count))))\n",
        "    id2word = {v : k for k,v in word2id.items()}\n",
        "    \n",
        "    return word2count, word2id, id2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:30.157872Z",
          "start_time": "2022-02-19T14:33:28.473330Z"
        },
        "id": "ieS5SiQx4WSm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 11156.84it/s]\n"
          ]
        }
      ],
      "source": [
        "word2count, word2id, id2word = make_vocab(docs, min_count=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:30.754722Z",
          "start_time": "2022-02-19T14:33:30.752115Z"
        },
        "id": "cT1MRN1EJtx6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "161,322\n"
          ]
        }
      ],
      "source": [
        "doc_len = sum(word2count.values()) # 문서 내 모든 단어의 개수 (단어별 등장 빈도의 총 합)\n",
        "print(f\"{doc_len:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'는': 0,\n",
              " '다': 1,\n",
              " '은': 2,\n",
              " '분류': 3,\n",
              " '있': 4,\n",
              " '고': 5,\n",
              " '한': 6,\n",
              " '되': 7,\n",
              " '었': 8,\n",
              " '도': 9,\n",
              " '적': 10,\n",
              " '인': 11,\n",
              " '기': 12,\n",
              " '했': 13,\n",
              " '역': 14,\n",
              " '학교': 15,\n",
              " '였': 16,\n",
              " '번': 17,\n",
              " '지': 18,\n",
              " '수': 19,\n",
              " '서울': 20,\n",
              " '게': 21,\n",
              " '한다': 22,\n",
              " '회': 23,\n",
              " '된': 24,\n",
              " '초등': 25,\n",
              " '며': 26,\n",
              " '할': 27,\n",
              " '대': 28,\n",
              " '대한민국': 29,\n",
              " '던': 30,\n",
              " '만': 31,\n",
              " '않': 32,\n",
              " '명': 33,\n",
              " '사람': 34,\n",
              " '해': 35,\n",
              " '한국': 36,\n",
              " '세': 37,\n",
              " '달인': 38,\n",
              " '중': 39,\n",
              " '전': 40,\n",
              " '된다': 41,\n",
              " '으며': 42,\n",
              " '았': 43,\n",
              " '시': 44,\n",
              " '받': 45,\n",
              " '성': 46,\n",
              " '면': 47,\n",
              " '사용': 48,\n",
              " '미국': 49,\n",
              " '라는': 50,\n",
              " '라고': 51,\n",
              " '고등학교': 52,\n",
              " '일본': 53,\n",
              " '화': 54,\n",
              " '선수': 55,\n",
              " '주': 56,\n",
              " '현대': 57,\n",
              " '대학교': 58,\n",
              " '개': 59,\n",
              " '같': 60,\n",
              " '두': 61,\n",
              " '센터': 62,\n",
              " '후': 63,\n",
              " '없': 64,\n",
              " '운행': 65,\n",
              " '말': 66,\n",
              " '때문': 67,\n",
              " '대한': 68,\n",
              " '미술': 69,\n",
              " '이후': 70,\n",
              " '대학': 71,\n",
              " '미술관': 72,\n",
              " '프랑스': 73,\n",
              " '축구': 74,\n",
              " '라': 75,\n",
              " '지역': 76,\n",
              " '는데': 77,\n",
              " '군': 78,\n",
              " '평일': 79,\n",
              " '위': 80,\n",
              " '선': 81,\n",
              " '중국': 82,\n",
              " '씨': 83,\n",
              " '면서': 84,\n",
              " '영국': 85,\n",
              " '다고': 86,\n",
              " '성향': 87,\n",
              " '살': 88,\n",
              " '달': 89,\n",
              " '현재': 90,\n",
              " '호': 91,\n",
              " '갤러리': 92,\n",
              " '전쟁': 93,\n",
              " '메이저': 94,\n",
              " '이름': 95,\n",
              " '보': 96,\n",
              " '많': 97,\n",
              " '매켄지': 98,\n",
              " '작가': 99,\n",
              " '베이': 100,\n",
              " '가지': 101,\n",
              " '밀스': 102,\n",
              " '최강': 103,\n",
              " '세계': 104,\n",
              " '화랑': 105,\n",
              " '국제': 106,\n",
              " '뷰': 107,\n",
              " '당시': 108,\n",
              " '다는': 109,\n",
              " '원': 110,\n",
              " '더': 111,\n",
              " '시작': 112,\n",
              " '팀': 113,\n",
              " '도전': 114,\n",
              " '권': 115,\n",
              " '위해': 116,\n",
              " '섬': 117,\n",
              " '터미널': 118,\n",
              " '영화': 119,\n",
              " '정': 120,\n",
              " '경우': 121,\n",
              " '진': 122,\n",
              " '차': 123,\n",
              " '세기': 124,\n",
              " '가장': 125,\n",
              " '출신': 126,\n",
              " '알': 127,\n",
              " '노선': 128,\n",
              " '집': 129,\n",
              " '즈': 130,\n",
              " '이나': 131,\n",
              " '동문': 132,\n",
              " '목표': 133,\n",
              " '방면': 134,\n",
              " '본': 135,\n",
              " '째': 136,\n",
              " '형': 137,\n",
              " '지방': 138,\n",
              " '상': 139,\n",
              " '서': 140,\n",
              " '도쿄': 141,\n",
              " '빌': 142,\n",
              " '태어남': 143,\n",
              " '계': 144,\n",
              " '국가': 145,\n",
              " '버스': 146,\n",
              " '사이': 147,\n",
              " '대전': 148,\n",
              " '만들': 149,\n",
              " '사회': 150,\n",
              " '보다': 151,\n",
              " '오스트레일리아': 152,\n",
              " '뒤': 153,\n",
              " '작품': 154,\n",
              " '조선': 155,\n",
              " '연구': 156,\n",
              " '일반': 157,\n",
              " '리그': 158,\n",
              " '국립': 159,\n",
              " '포함': 160,\n",
              " '남': 161,\n",
              " '부': 162,\n",
              " '활동': 163,\n",
              " '마컴': 164,\n",
              " '는다': 165,\n",
              " '안': 166,\n",
              " '함': 167,\n",
              " '기록': 168,\n",
              " '편': 169,\n",
              " '까치': 170,\n",
              " '랑': 171,\n",
              " '토마스': 172,\n",
              " '위치': 173,\n",
              " '내': 174,\n",
              " '배서스트': 175,\n",
              " '교육': 176,\n",
              " '졸업': 177,\n",
              " '여자': 178,\n",
              " '신': 179,\n",
              " '비': 180,\n",
              " '정도': 181,\n",
              " '식': 182,\n",
              " '위한': 183,\n",
              " '통해': 184,\n",
              " '대표': 185,\n",
              " '지원': 186,\n",
              " '스': 187,\n",
              " '스틸': 188,\n",
              " '간': 189,\n",
              " '부산': 190,\n",
              " '데': 191,\n",
              " '드': 192,\n",
              " '로서': 193,\n",
              " '드라마': 194,\n",
              " '협회': 195,\n",
              " '아트': 196,\n",
              " '모델': 197,\n",
              " '초': 198,\n",
              " '존': 199,\n",
              " '리치먼드힐': 200,\n",
              " '길': 201,\n",
              " '설립': 202,\n",
              " '시대': 203,\n",
              " '이러': 204,\n",
              " '왕': 205,\n",
              " '곳': 206,\n",
              " '방송': 207,\n",
              " '육군': 208,\n",
              " '개발': 209,\n",
              " '으나': 210,\n",
              " '크': 211,\n",
              " '사건': 212,\n",
              " '학급': 213,\n",
              " '또는': 214,\n",
              " '대회': 215,\n",
              " '당': 216,\n",
              " '조': 217,\n",
              " '주장': 218,\n",
              " '나라': 219,\n",
              " '배우': 220,\n",
              " '처럼': 221,\n",
              " '다시': 222,\n",
              " '텔레비전': 223,\n",
              " '될': 224,\n",
              " '첫': 225,\n",
              " '대해': 226,\n",
              " '학습': 227,\n",
              " '존재': 228,\n",
              " '족': 229,\n",
              " '돈': 230,\n",
              " '뉴마켓': 231,\n",
              " '일부': 232,\n",
              " '대상': 233,\n",
              " '레슬리': 234,\n",
              " '가능': 235,\n",
              " '타': 236,\n",
              " '결과': 237,\n",
              " '위원회': 238,\n",
              " '핀치': 239,\n",
              " '여성': 240,\n",
              " '정부': 241,\n",
              " '아들': 242,\n",
              " '경기': 243,\n",
              " '역사': 244,\n",
              " '기술': 245,\n",
              " '교수': 246,\n",
              " '모든': 247,\n",
              " '새': 248,\n",
              " '관계': 249,\n",
              " '오후': 250,\n",
              " '밸리': 251,\n",
              " '몰': 252,\n",
              " '슈퍼': 253,\n",
              " '문화': 254,\n",
              " '발견': 255,\n",
              " '대구': 256,\n",
              " '지정': 257,\n",
              " '현': 258,\n",
              " '정치': 259,\n",
              " '아퀴나스': 260,\n",
              " '거나': 261,\n",
              " '엘긴': 262,\n",
              " '부분': 263,\n",
              " '로드': 264,\n",
              " '아침': 265,\n",
              " '장': 266,\n",
              " '힐': 267,\n",
              " '도시': 268,\n",
              " '홍콩': 269,\n",
              " '러더퍼드': 270,\n",
              " '큰': 271,\n",
              " '제국': 272,\n",
              " '전투': 273,\n",
              " '파리': 274,\n",
              " '구조': 275,\n",
              " '워든': 276,\n",
              " '러시': 277,\n",
              " '처음': 278,\n",
              " '높': 279,\n",
              " '예술': 280,\n",
              " '대부분': 281,\n",
              " '발매': 282,\n",
              " '론': 283,\n",
              " '요새': 284,\n",
              " '선거구': 285,\n",
              " '우드바인': 286,\n",
              " '피': 287,\n",
              " '게임': 288,\n",
              " '여러': 289,\n",
              " '사랑': 290,\n",
              " '약': 291,\n",
              " '아워': 292,\n",
              " '연옥': 293,\n",
              " '계획': 294,\n",
              " '강': 295,\n",
              " '그룹': 296,\n",
              " '정차': 297,\n",
              " '발음': 298,\n",
              " '죽음': 299,\n",
              " '손': 300,\n",
              " '가톨릭': 301,\n",
              " '독일': 302,\n",
              " '점': 303,\n",
              " '공식': 304,\n",
              " '특히': 305,\n",
              " '정보': 306,\n",
              " '표준': 307,\n",
              " '과학': 308,\n",
              " '이용': 309,\n",
              " '주최': 310,\n",
              " '이론': 311,\n",
              " '아버지': 312,\n",
              " '유럽': 313,\n",
              " '시키': 314,\n",
              " '컵': 315,\n",
              " '설치': 316,\n",
              " '평가': 317,\n",
              " '중심': 318,\n",
              " '딸': 319,\n",
              " '구성': 320,\n",
              " '생활': 321,\n",
              " '어서': 322,\n",
              " '속': 323,\n",
              " '죽': 324,\n",
              " '운영': 325,\n",
              " '개념': 326,\n",
              " '수학': 327,\n",
              " '최초': 328,\n",
              " '싱글': 329,\n",
              " '생각': 330,\n",
              " '프로': 331,\n",
              " '+': 332,\n",
              " '불': 333,\n",
              " '추가': 334,\n",
              " '다가': 335,\n",
              " '음': 336,\n",
              " '총': 337,\n",
              " '시즌': 338,\n",
              " '문제': 339,\n",
              " '무': 340,\n",
              " '방식': 341,\n",
              " '공간': 342,\n",
              " '동': 343,\n",
              " '조선총독부': 344,\n",
              " '매우': 345,\n",
              " '필요': 346,\n",
              " '의미': 347,\n",
              " '관련': 348,\n",
              " '이탈리아': 349,\n",
              " '대통령': 350,\n",
              " '표현': 351,\n",
              " '매코': 352,\n",
              " '시기': 353,\n",
              " '앤': 354,\n",
              " '행정': 355,\n",
              " '젠더': 356,\n",
              " '매일': 357,\n",
              " '세인트': 358,\n",
              " '킬': 359,\n",
              " '했으며': 360,\n",
              " '였으며': 361,\n",
              " '다양': 362,\n",
              " '트': 363,\n",
              " '사망': 364,\n",
              " '란': 365,\n",
              " '특별': 366,\n",
              " '미니': 367,\n",
              " '능력': 368,\n",
              " '중화': 369,\n",
              " '조직': 370,\n",
              " '최고': 371,\n",
              " '올림픽': 372,\n",
              " '천': 373,\n",
              " '변경': 374,\n",
              " '음악': 375,\n",
              " '좋': 376,\n",
              " '구간': 377,\n",
              " '과정': 378,\n",
              " '개인': 379,\n",
              " '공격': 380,\n",
              " '마운트': 381,\n",
              " '분': 382,\n",
              " '프로그램': 383,\n",
              " '기념': 384,\n",
              " '날': 385,\n",
              " '노래': 386,\n",
              " '코오롱': 387,\n",
              " '기업': 388,\n",
              " '제도': 389,\n",
              " '선거': 390,\n",
              " '맥': 391,\n",
              " '맛': 392,\n",
              " '대부': 393,\n",
              " '상시': 394,\n",
              " '내용': 395,\n",
              " '로마': 396,\n",
              " '상태': 397,\n",
              " '곡': 398,\n",
              " '이전': 399,\n",
              " '인텔': 400,\n",
              " '니': 401,\n",
              " '뿐': 402,\n",
              " '새로운': 403,\n",
              " '경제': 404,\n",
              " '제작': 405,\n",
              " '요리': 406,\n",
              " '중학교': 407,\n",
              " '산업': 408,\n",
              " '경기도': 409,\n",
              " '어머니': 410,\n",
              " '영향': 411,\n",
              " '용': 412,\n",
              " '이유': 413,\n",
              " '연결': 414,\n",
              " '스페인': 415,\n",
              " '역할': 416,\n",
              " '철도': 417,\n",
              " '웰링턴': 418,\n",
              " '목록': 419,\n",
              " '테': 420,\n",
              " '인간': 421,\n",
              " '사업': 422,\n",
              " '케네디': 423,\n",
              " '재': 424,\n",
              " '문': 425,\n",
              " '많이': 426,\n",
              " '생': 427,\n",
              " '억': 428,\n",
              " '가수': 429,\n",
              " '철도역': 430,\n",
              " '학생': 431,\n",
              " '사무관': 432,\n",
              " '스토우': 433,\n",
              " '박상연': 434,\n",
              " '잘': 435,\n",
              " '키': 436,\n",
              " '도록': 437,\n",
              " '초기': 438,\n",
              " '해서': 439,\n",
              " '쓰': 440,\n",
              " '자연': 441,\n",
              " '잡': 442,\n",
              " '인구': 443,\n",
              " '인디언': 444,\n",
              " '후쿠오카': 445,\n",
              " '킹': 446,\n",
              " '은둔': 447,\n",
              " '목': 448,\n",
              " '김': 449,\n",
              " '리': 450,\n",
              " '지구': 451,\n",
              " '배': 452,\n",
              " '사실': 453,\n",
              " '작': 454,\n",
              " '방법': 455,\n",
              " '관리': 456,\n",
              " '소속': 457,\n",
              " '중요': 458,\n",
              " '변화': 459,\n",
              " '학회': 460,\n",
              " '퇴임': 461,\n",
              " '웨스턴': 462,\n",
              " '더퍼린': 463,\n",
              " '외': 464,\n",
              " '더불': 465,\n",
              " '대신': 466,\n",
              " '뜻': 467,\n",
              " '열': 468,\n",
              " '스타': 469,\n",
              " '시장': 470,\n",
              " '볼': 471,\n",
              " '참여': 472,\n",
              " '의원': 473,\n",
              " '단체': 474,\n",
              " '베이징': 475,\n",
              " '공주': 476,\n",
              " '초대': 477,\n",
              " '우승': 478,\n",
              " '인지': 479,\n",
              " '음반': 480,\n",
              " '맡': 481,\n",
              " '분야': 482,\n",
              " '고려': 483,\n",
              " '턴': 484,\n",
              " '곤살로': 485,\n",
              " '중추원': 486,\n",
              " '그로브': 487,\n",
              " '남자': 488,\n",
              " '헝가리': 489,\n",
              " '칼': 490,\n",
              " '끝': 491,\n",
              " '머리': 492,\n",
              " '앨범': 493,\n",
              " '카': 494,\n",
              " '한편': 495,\n",
              " '중앙': 496,\n",
              " '캐나다': 497,\n",
              " '코어': 498,\n",
              " '색': 499,\n",
              " '컴퓨터': 500,\n",
              " '아래': 501,\n",
              " '편성': 502,\n",
              " '구역': 503,\n",
              " '군사': 504,\n",
              " '계속': 505,\n",
              " '병원': 506,\n",
              " '데이터': 507,\n",
              " '주요': 508,\n",
              " '인민공화국': 509,\n",
              " '공항': 510,\n",
              " '디스플레이': 511,\n",
              " '시스템': 512,\n",
              " '가진': 513,\n",
              " '얻': 514,\n",
              " '거주': 515,\n",
              " '모습': 516,\n",
              " '수록': 517,\n",
              " '만화': 518,\n",
              " '스포츠': 519,\n",
              " '조사': 520,\n",
              " '민주당': 521,\n",
              " '취임': 522,\n",
              " '샘터': 523,\n",
              " '제공': 524,\n",
              " '교장': 525,\n",
              " '회장': 526,\n",
              " '제인': 527,\n",
              " '반': 528,\n",
              " '애니메이션': 529,\n",
              " '감독': 530,\n",
              " '등장': 531,\n",
              " '양': 532,\n",
              " '전체': 533,\n",
              " '타운': 534,\n",
              " '광주': 535,\n",
              " '버전': 536,\n",
              " '병': 537,\n",
              " '사운드': 538,\n",
              " '글': 539,\n",
              " '개교': 540,\n",
              " '경상북도': 541,\n",
              " '기준': 542,\n",
              " '단': 543,\n",
              " '엔': 544,\n",
              " '됐': 545,\n",
              " '분석': 546,\n",
              " '운동': 547,\n",
              " '뉴욕': 548,\n",
              " '러시아': 549,\n",
              " '상황': 550,\n",
              " '향': 551,\n",
              " '마산역': 552,\n",
              " '제주시': 553,\n",
              " '따르': 554,\n",
              " '서로': 555,\n",
              " '유일': 556,\n",
              " '산': 557,\n",
              " '회사': 558,\n",
              " '진행': 559,\n",
              " '인해': 560,\n",
              " '헨리': 561,\n",
              " '추정': 562,\n",
              " '통합': 563,\n",
              " '검': 564,\n",
              " '물': 565,\n",
              " '전용': 566,\n",
              " '행위': 567,\n",
              " '집합': 568,\n",
              " '승차': 569,\n",
              " '프랑스군': 570,\n",
              " '참가': 571,\n",
              " '인가': 572,\n",
              " '눈': 573,\n",
              " '커피': 574,\n",
              " '이야기': 575,\n",
              " '빵': 576,\n",
              " '나이': 577,\n",
              " '발생': 578,\n",
              " '토': 579,\n",
              " '앞': 580,\n",
              " '통': 581,\n",
              " '서브': 582,\n",
              " '행동': 583,\n",
              " '영토': 584,\n",
              " '열차': 585,\n",
              " '포장': 586,\n",
              " '파크': 587,\n",
              " '검사': 588,\n",
              " '기원전': 589,\n",
              " '잉글랜드': 590,\n",
              " '보이': 591,\n",
              " '했으나': 592,\n",
              " '청년': 593,\n",
              " '백': 594,\n",
              " '종': 595,\n",
              " '확인': 596,\n",
              " '형태': 597,\n",
              " '나무': 598,\n",
              " '특수': 599,\n",
              " '주로': 600,\n",
              " '자유': 601,\n",
              " '야구': 602,\n",
              " '재산': 603,\n",
              " '왕국': 604,\n",
              " '주연': 605,\n",
              " '박서보': 606,\n",
              " '그린': 607,\n",
              " '마지막': 608,\n",
              " '졸업식': 609,\n",
              " '조지': 610,\n",
              " '노': 611,\n",
              " '윌리엄': 612,\n",
              " '치': 613,\n",
              " '차이': 614,\n",
              " '전설': 615,\n",
              " '성공': 616,\n",
              " '유지': 617,\n",
              " '동물': 618,\n",
              " '정책': 619,\n",
              " '인물': 620,\n",
              " '해당': 621,\n",
              " '바': 622,\n",
              " '식민지': 623,\n",
              " '성과': 624,\n",
              " '설정': 625,\n",
              " '결혼': 626,\n",
              " '교회': 627,\n",
              " '영혼': 628,\n",
              " '디지털': 629,\n",
              " '만든': 630,\n",
              " '상대': 631,\n",
              " '법': 632,\n",
              " '특성': 633,\n",
              " '클럽': 634,\n",
              " '뉴스': 635,\n",
              " '자리': 636,\n",
              " '랭': 637,\n",
              " '그녀': 638,\n",
              " '장치': 639,\n",
              " '그것': 640,\n",
              " '공립': 641,\n",
              " '홈페이지': 642,\n",
              " '혼인': 643,\n",
              " '진출': 644,\n",
              " '갖': 645,\n",
              " '시리즈': 646,\n",
              " '유명': 647,\n",
              " '으로부터': 648,\n",
              " '발표': 649,\n",
              " '이르': 650,\n",
              " '교통': 651,\n",
              " '구분': 652,\n",
              " '개관': 653,\n",
              " '대수': 654,\n",
              " '페트라': 655,\n",
              " '우드': 656,\n",
              " '버나드': 657,\n",
              " '해야': 658,\n",
              " '전기': 659,\n",
              " '본선': 660,\n",
              " '못하': 661,\n",
              " '국': 662,\n",
              " '발전': 663,\n",
              " '수사': 664,\n",
              " '그림': 665,\n",
              " '수상': 666,\n",
              " '메인': 667,\n",
              " '우주': 668,\n",
              " '선택': 669,\n",
              " '지금': 670,\n",
              " '박': 671,\n",
              " '링크': 672,\n",
              " '하카타': 673,\n",
              " '생산': 674,\n",
              " '류': 675,\n",
              " '찾': 676,\n",
              " '환경': 677,\n",
              " '온': 678,\n",
              " '줄': 679,\n",
              " '버': 680,\n",
              " '공개': 681,\n",
              " '영어': 682,\n",
              " '미': 683,\n",
              " '통일': 684,\n",
              " '가선': 685,\n",
              " '음력': 686,\n",
              " '규모': 687,\n",
              " '양성': 688,\n",
              " '위원장': 689,\n",
              " '미래': 690,\n",
              " '래피드': 691,\n",
              " '작업': 692,\n",
              " '벡터': 693,\n",
              " '스태프': 694,\n",
              " '부족': 695,\n",
              " '소': 696,\n",
              " '체제': 697,\n",
              " '항': 698,\n",
              " '전통': 699,\n",
              " '종이': 700,\n",
              " '비교': 701,\n",
              " '땅': 702,\n",
              " '적용': 703,\n",
              " '국회의원': 704,\n",
              " '국민': 705,\n",
              " '수행': 706,\n",
              " '마이너': 707,\n",
              " '숨': 708,\n",
              " '독립': 709,\n",
              " '최': 710,\n",
              " '장소': 711,\n",
              " '하차': 712,\n",
              " '오하이오': 713,\n",
              " '워싱턴': 714,\n",
              " '윤': 715,\n",
              " '지하철': 716,\n",
              " '중화민국': 717,\n",
              " '애플': 718,\n",
              " '전라북도': 719,\n",
              " '장관': 720,\n",
              " '크로마': 721,\n",
              " '보안': 722,\n",
              " '못했': 723,\n",
              " '맨': 724,\n",
              " '불리': 725,\n",
              " '결정': 726,\n",
              " '명칭': 727,\n",
              " '섬네일': 728,\n",
              " '먼': 729,\n",
              " '영상': 730,\n",
              " '티': 731,\n",
              " '폼': 732,\n",
              " '경상남도': 733,\n",
              " '업무': 734,\n",
              " '민족': 735,\n",
              " '승강장': 736,\n",
              " '아시아': 737,\n",
              " '삼성': 738,\n",
              " '학자': 739,\n",
              " '화물': 740,\n",
              " '선생': 741,\n",
              " '마산': 742,\n",
              " '개체': 743,\n",
              " '톤': 744,\n",
              " '기능': 745,\n",
              " '이미': 746,\n",
              " '시켰': 747,\n",
              " '경': 748,\n",
              " '동생': 749,\n",
              " '비롯': 750,\n",
              " '됨': 751,\n",
              " '마다': 752,\n",
              " '아종': 753,\n",
              " '지나': 754,\n",
              " '브': 755,\n",
              " '작곡': 756,\n",
              " '없이': 757,\n",
              " '기간': 758,\n",
              " '가족': 759,\n",
              " '과장': 760,\n",
              " '르': 761,\n",
              " '법원': 762,\n",
              " '자전거': 763,\n",
              " '마틴': 764,\n",
              " '샘플링': 765,\n",
              " '봉성군': 766,\n",
              " '멀록': 767,\n",
              " '바이저': 768,\n",
              " '떡볶이': 769,\n",
              " '붉': 770,\n",
              " '포': 771,\n",
              " '달리': 772,\n",
              " '카드': 773,\n",
              " '확장': 774,\n",
              " '비슷': 775,\n",
              " '가운데': 776,\n",
              " '밤': 777,\n",
              " '버리': 778,\n",
              " '생존': 779,\n",
              " '졌': 780,\n",
              " '거리': 781,\n",
              " '외국인': 782,\n",
              " '용어': 783,\n",
              " '데뷔': 784,\n",
              " '왔': 785,\n",
              " '형성': 786,\n",
              " '조정': 787,\n",
              " '전국': 788,\n",
              " '담당': 789,\n",
              " '건': 790,\n",
              " '위원': 791,\n",
              " '기획': 792,\n",
              " '포르투갈': 793,\n",
              " '번지': 794,\n",
              " '미드': 795,\n",
              " '웨스트': 796,\n",
              " '크리크': 797,\n",
              " '이즐링턴': 798,\n",
              " '메트로': 799,\n",
              " '케': 800,\n",
              " '루': 801,\n",
              " '등록': 802,\n",
              " '기본': 803,\n",
              " '계열': 804,\n",
              " '제조': 805,\n",
              " '역시': 806,\n",
              " '분교': 807,\n",
              " '이루': 808,\n",
              " '의한': 809,\n",
              " '발': 810,\n",
              " '원래': 811,\n",
              " '대사': 812,\n",
              " '건설': 813,\n",
              " '과거': 814,\n",
              " '개시': 815,\n",
              " '연방': 816,\n",
              " '공화국': 817,\n",
              " '꽃': 818,\n",
              " '츠': 819,\n",
              " '정리': 820,\n",
              " '전철': 821,\n",
              " '기독교': 822,\n",
              " '밥': 823,\n",
              " '마크': 824,\n",
              " '특집': 825,\n",
              " '기저': 826,\n",
              " '정의': 827,\n",
              " '설계': 828,\n",
              " '효과': 829,\n",
              " '불린': 830,\n",
              " '마을': 831,\n",
              " '막': 832,\n",
              " '세상': 833,\n",
              " '안전': 834,\n",
              " '채': 835,\n",
              " '모양': 836,\n",
              " '부문': 837,\n",
              " '자료': 838,\n",
              " '마리': 839,\n",
              " '디자인': 840,\n",
              " '달러': 841,\n",
              " '비디오': 842,\n",
              " '레': 843,\n",
              " '남쪽': 844,\n",
              " '김영삼': 845,\n",
              " '창': 846,\n",
              " '도지사': 847,\n",
              " '데이비스': 848,\n",
              " '본부': 849,\n",
              " '포트': 850,\n",
              " '빌리지': 851,\n",
              " '오로라': 852,\n",
              " '보통': 853,\n",
              " '주석': 854,\n",
              " '부르': 855,\n",
              " '출시': 856,\n",
              " '연재': 857,\n",
              " '의하': 858,\n",
              " '마사키': 859,\n",
              " '엔터': 860,\n",
              " '제외': 861,\n",
              " '폐지': 862,\n",
              " '라면': 863,\n",
              " '싶': 864,\n",
              " '조약': 865,\n",
              " '목적': 866,\n",
              " '가치': 867,\n",
              " '실패': 868,\n",
              " '진주': 869,\n",
              " '는지': 870,\n",
              " '박사': 871,\n",
              " '비고': 872,\n",
              " '주말': 873,\n",
              " '과업': 874,\n",
              " '죄': 875,\n",
              " '주민': 876,\n",
              " '신호': 877,\n",
              " '함부르크': 878,\n",
              " '수도사': 879,\n",
              " '애': 880,\n",
              " '기라티나': 881,\n",
              " '바젤': 882,\n",
              " '드라이브': 883,\n",
              " '준': 884,\n",
              " '소프트웨어': 885,\n",
              " '반면': 886,\n",
              " '멸망': 887,\n",
              " '님': 888,\n",
              " '황제': 889,\n",
              " '고대': 890,\n",
              " '기반': 891,\n",
              " '폴': 892,\n",
              " '스페셜': 893,\n",
              " '개통': 894,\n",
              " '이끌': 895,\n",
              " '종합': 896,\n",
              " '넘': 897,\n",
              " '후보': 898,\n",
              " '활용': 899,\n",
              " '후지': 900,\n",
              " '기구': 901,\n",
              " '정당': 902,\n",
              " '소리': 903,\n",
              " '공원': 904,\n",
              " '백서': 905,\n",
              " '성인': 906,\n",
              " '로봇': 907,\n",
              " '부리': 908,\n",
              " '스트리트': 909,\n",
              " '오크': 910,\n",
              " '헌법': 911,\n",
              " '만두': 912,\n",
              " '자영업': 913,\n",
              " '아닌': 914,\n",
              " '남성': 915,\n",
              " '선정': 916,\n",
              " '홍': 917,\n",
              " '파일': 918,\n",
              " '해군': 919,\n",
              " '다면': 920,\n",
              " '차원': 921,\n",
              " '요구': 922,\n",
              " '런던': 923,\n",
              " '원정': 924,\n",
              " '맞': 925,\n",
              " '우': 926,\n",
              " '알려져': 927,\n",
              " '설명': 928,\n",
              " '밀': 929,\n",
              " '밖': 930,\n",
              " '보내': 931,\n",
              " '먹': 932,\n",
              " '집단': 933,\n",
              " '트리': 934,\n",
              " '소유': 935,\n",
              " '출연': 936,\n",
              " '군인': 937,\n",
              " '특별시': 938,\n",
              " '보여': 939,\n",
              " '정규': 940,\n",
              " '작전': 941,\n",
              " '전주': 942,\n",
              " '여객철도': 943,\n",
              " '서부': 944,\n",
              " '올': 945,\n",
              " '경유': 946,\n",
              " '의회': 947,\n",
              " '수도': 948,\n",
              " '창원시': 949,\n",
              " '공병': 950,\n",
              " '펜': 951,\n",
              " '식물': 952,\n",
              " '루마니아': 953,\n",
              " '정화': 954,\n",
              " '육룡': 955,\n",
              " '전주시': 956,\n",
              " '표': 957,\n",
              " '차례': 958,\n",
              " '관할': 959,\n",
              " '개월': 960,\n",
              " '태어났': 961,\n",
              " '못': 962,\n",
              " '쇼': 963,\n",
              " '연합': 964,\n",
              " '입': 965,\n",
              " '크기': 966,\n",
              " '노력': 967,\n",
              " '제품': 968,\n",
              " '민주': 969,\n",
              " '도입': 970,\n",
              " '시민': 971,\n",
              " '그대로': 972,\n",
              " '어야': 973,\n",
              " '높이': 974,\n",
              " '한다는': 975,\n",
              " '문화재': 976,\n",
              " '성장': 977,\n",
              " '체로키': 978,\n",
              " '힘': 979,\n",
              " '시립': 980,\n",
              " '성별': 981,\n",
              " '회의': 982,\n",
              " '인정': 983,\n",
              " '송': 984,\n",
              " '전파': 985,\n",
              " '장군': 986,\n",
              " '회관': 987,\n",
              " '클리어타입': 988,\n",
              " '클라크': 989,\n",
              " '버치': 990,\n",
              " '커넥터': 991,\n",
              " '집합족': 992,\n",
              " '배치': 993,\n",
              " '코': 994,\n",
              " '예정': 995,\n",
              " '분리': 996,\n",
              " '짧': 997,\n",
              " '사진': 998,\n",
              " '였으나': 999,\n",
              " ...}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:32.916830Z",
          "start_time": "2022-02-19T14:33:32.914355Z"
        },
        "id": "e_1MneB54WSm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# unique word : 5,662\n"
          ]
        }
      ],
      "source": [
        "print(f\"# unique word : {len(word2id):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHxtZqtk4WSm"
      },
      "source": [
        "### Dataset 클래스 구현\n",
        "- Skip-Gram 방식의 학습 데이터 셋(`Tuple(target_word, context_word)`)을 생성하는 `CustomDataset` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()` 함수) 입력 매개변수\n",
        "        - docs: 문서 리스트\n",
        "        - word2id: 단어별 인덱스(wid) 사전\n",
        "        - window_size: Skip-Gram의 윈도우 사이즈\n",
        "    - 메소드\n",
        "        - `make_pair()`\n",
        "            - 문서를 단어로 쪼개고, 사전에 존재하는 단어들만 단어 인덱스로 변경\n",
        "            - Skip-gram 방식의 `(target_word, context_word)` 페어(tuple)들을 `pairs` 리스트에 담아 반환\n",
        "        - `__len__()`\n",
        "            - `pairs` 리스트의 개수 반환\n",
        "        - `__getitem__(index)`\n",
        "            - `pairs` 리스트를 인덱싱\n",
        "    - 주의 사항\n",
        "        - `nn.Module`를 부모 클래스로 상속 받음 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.111290Z",
          "start_time": "2022-02-19T14:33:38.104531Z"
        },
        "id": "UPiLcYCZ4WSm"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    문서 리스트를 받아 skip-gram 방식의 (target_word, context_word) 데이터 셋을 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, docs:List[str], word2id:Dict[str,int], window_size:int=5):\n",
        "        # 생성자 입력 매개 변수 할당\n",
        "        self.docs = docs\n",
        "        self.word2id = word2id\n",
        "        self.window_size = window_size\n",
        "        self.pairs = self.make_pair()\n",
        "    \n",
        "    def make_pair(self):\n",
        "        \"\"\"\n",
        "        (target, context) 형식의 Skip-gram pair 데이터 셋 생성 \n",
        "        \"\"\"\n",
        "        pairs = []\n",
        "\n",
        "        for doc in docs:\n",
        "            count = 0\n",
        "            word_list = doc.split()\n",
        "            word_idx_list = []\n",
        "            #word 접근하면서 word2id에 있는지 확인\n",
        "            for word in word_list:\n",
        "                if word not in word2id:\n",
        "                    continue\n",
        "                word_idx_list.append(word2id[word])\n",
        "            # (target, context) pair 생성\n",
        "            for i, word_idx in enumerate(word_idx_list):\n",
        "                # 최대, 최소 idx 설정\n",
        "                max_idx = min(i + self.window_size, len(word_idx_list))\n",
        "                min_idx = max(0, i - self.window_size)\n",
        "                # idx 돌면서 pair 생성\n",
        "                for j in range(min_idx, max_idx + 1):\n",
        "                    # 자기 자신 제외, 범위 넘어가는 것 제외\n",
        "                    if j == i or j >= len(word_idx_list):\n",
        "                        continue\n",
        "                    idx = min(j, max_idx - 1)\n",
        "                    pairs.append((word_idx, word_idx_list[idx]))\n",
        "\n",
        "        return pairs\n",
        "        \n",
        "    def __len__(self):\n",
        "        # pairs 길이 반환\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # pairs의 idx 반환\n",
        "        return self.pairs[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.945361Z",
          "start_time": "2022-02-19T14:33:38.385577Z"
        },
        "id": "YntOw2q94WSm"
      },
      "outputs": [],
      "source": [
        "dataset = CustomDataset(docs, word2id, window_size=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.949614Z",
          "start_time": "2022-02-19T14:33:38.946663Z"
        },
        "id": "-RpNbAjk4WSn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1598220"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:43.072635Z",
          "start_time": "2022-02-19T14:33:43.069526Z"
        },
        "id": "1FBwcL4H4WSn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4151, 476)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:51.040595Z",
          "start_time": "2022-02-19T14:33:51.031473Z"
        },
        "id": "wTAwTjKk4WSn",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(남모, 공주)\n",
            "(남모, 는)\n",
            "(남모, 신라)\n",
            "(남모, 공주)\n",
            "(남모, 공주)\n",
            "(공주, 남모)\n",
            "(공주, 는)\n",
            "(공주, 신라)\n",
            "(공주, 공주)\n",
            "(공주, 왕족)\n",
            "(공주, 왕족)\n",
            "(는, 남모)\n",
            "(는, 공주)\n",
            "(는, 신라)\n",
            "(는, 공주)\n",
            "(는, 왕족)\n",
            "(는, 공주)\n",
            "(는, 공주)\n",
            "(신라, 남모)\n",
            "(신라, 공주)\n",
            "(신라, 는)\n",
            "(신라, 공주)\n",
            "(신라, 왕족)\n",
            "(신라, 공주)\n",
            "(신라, 부여)\n",
            "(신라, 부여)\n",
            "(공주, 남모)\n",
            "(공주, 공주)\n",
            "(공주, 는)\n",
            "(공주, 신라)\n",
            "(공주, 왕족)\n",
            "(공주, 공주)\n",
            "(공주, 부여)\n",
            "(공주, 씨)\n",
            "(공주, 씨)\n",
            "(왕족, 남모)\n",
            "(왕족, 공주)\n",
            "(왕족, 는)\n",
            "(왕족, 신라)\n",
            "(왕족, 공주)\n",
            "(왕족, 공주)\n",
            "(왕족, 부여)\n",
            "(왕족, 씨)\n",
            "(왕족, 딸)\n",
            "(왕족, 딸)\n",
            "(공주, 공주)\n",
            "(공주, 는)\n",
            "(공주, 신라)\n",
            "(공주, 공주)\n",
            "(공주, 왕족)\n",
            "(공주, 부여)\n",
            "(공주, 씨)\n",
            "(공주, 딸)\n",
            "(공주, 며)\n",
            "(공주, 며)\n",
            "(부여, 는)\n",
            "(부여, 신라)\n",
            "(부여, 공주)\n",
            "(부여, 왕족)\n",
            "(부여, 공주)\n",
            "(부여, 씨)\n",
            "(부여, 딸)\n",
            "(부여, 며)\n",
            "(부여, 백제)\n",
            "(부여, 백제)\n",
            "(씨, 신라)\n",
            "(씨, 공주)\n",
            "(씨, 왕족)\n",
            "(씨, 공주)\n",
            "(씨, 부여)\n",
            "(씨, 딸)\n",
            "(씨, 며)\n",
            "(씨, 백제)\n",
            "(씨, 였)\n",
            "(씨, 였)\n",
            "(딸, 공주)\n",
            "(딸, 왕족)\n",
            "(딸, 공주)\n",
            "(딸, 부여)\n",
            "(딸, 씨)\n",
            "(딸, 며)\n",
            "(딸, 백제)\n",
            "(딸, 였)\n",
            "(딸, 다)\n",
            "(딸, 다)\n",
            "(며, 왕족)\n",
            "(며, 공주)\n",
            "(며, 부여)\n",
            "(며, 씨)\n",
            "(며, 딸)\n",
            "(며, 백제)\n",
            "(며, 였)\n",
            "(며, 다)\n",
            "(며, 인)\n",
            "(며, 인)\n",
            "(백제, 공주)\n",
            "(백제, 부여)\n",
            "(백제, 씨)\n",
            "(백제, 딸)\n",
            "(백제, 며)\n"
          ]
        }
      ],
      "source": [
        "# verify (target word, context word)\n",
        "for i, pair in enumerate(dataset):\n",
        "    if i==100:\n",
        "        break\n",
        "    print(f\"({id2word[pair[0]]}, {id2word[pair[1]]})\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Z50-Dr4WSn"
      },
      "source": [
        "### 위에서 생성한 `dataset`으로 DataLoader  객체 생성\n",
        "- `DataLoader` 클래스로 `train_dataloader`객체를 생성하라. \n",
        "    - 생성자 매개변수와 값\n",
        "        - dataset = 위에서 생성한 dataset\n",
        "        - batch_size = 64\n",
        "        - shuffle = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:02.645176Z",
          "start_time": "2022-02-19T14:34:02.642780Z"
        },
        "id": "GXcAvFB14WSn"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(dataset, 64, shuffle = True, generator=torch.manual_seed(seed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:02.777322Z",
          "start_time": "2022-02-19T14:34:02.774335Z"
        },
        "id": "4Yfcwi_14WSn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24973"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTs16gsU4WSn"
      },
      "source": [
        "### Negative Sampling 함수 구현\n",
        "- Skip-Gram은 복잡도를 줄이기 위한 방법으로 negative sampling을 사용한다. \n",
        "- `sample_table`이 다음과 같이 주어졌을 때, sample_table에서 랜덤으로 값을 뽑아 (batch_size, n_neg_sample) shape의 matrix를 반환하는 `get_neg_v_negative_sampling()`함수를 구현하라. \n",
        "- Sample Table은 negative distribution을 따른다. \n",
        "    - [negative distribution 설명](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#How-are-negative-samples-drawn?)\n",
        "- 함수 정의\n",
        "    - 입력 매개변수\n",
        "        - batch_size : 배치 사이즈, matrix의 row 개수 \n",
        "        - n_neg_sample : negative sample의 개수, matrix의 column 개수\n",
        "    - 반환값 \n",
        "        - neg_v : 추출된 negative sample (2차원의 리스트)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.397509Z",
          "start_time": "2022-02-19T14:34:11.386389Z"
        },
        "id": "PUqIB6dH4WSn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# negative sample을 추출할 sample table 생성 (해당 코드를 참고)\n",
        "sample_table = []\n",
        "sample_table_size = doc_len\n",
        "\n",
        "# noise distribution 생성\n",
        "alpha = 3/4\n",
        "frequency_list = np.array(list(word2count.values())) ** alpha\n",
        "Z = sum(frequency_list)\n",
        "ratio = frequency_list/Z\n",
        "negative_sample_dist = np.round(ratio*sample_table_size)\n",
        "\n",
        "for wid, c in enumerate(negative_sample_dist):\n",
        "    sample_table.extend([wid]*int(c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.508414Z",
          "start_time": "2022-02-19T14:34:11.505464Z"
        },
        "id": "Wdu8qK8x4WSn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "161159"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sample_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.656046Z",
          "start_time": "2022-02-19T14:34:11.653325Z"
        },
        "id": "mQIVrOIR4WSn"
      },
      "outputs": [],
      "source": [
        "def get_neg_v_negative_sampling(batch_size:int, n_neg_sample:int):\n",
        "    \"\"\"\n",
        "    위에서 정의한 sample_table에서 (batch_size, n_neg_sample) shape만큼 랜덤 추출해 \"네거티브 샘플 메트릭스\"를 생성\n",
        "    np.random.choice() 함수 활용 (위에서 정의한 sample_table을 함수의 argument로 사용)\n",
        "    \"\"\"\n",
        "    neg_v = np.random.choice(sample_table, size = (batch_size, n_neg_sample))\n",
        "    \n",
        "    return neg_v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:12.345976Z",
          "start_time": "2022-02-19T14:34:12.333448Z"
        },
        "id": "8wwT4Af04WSo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   8, 3669,   14, 1963, 1232],\n",
              "       [5526,  109, 3391,  140, 5352],\n",
              "       [2632, 4179,  672,   90,   65],\n",
              "       [  17, 2613,  937, 4281,  886]])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_neg_v_negative_sampling(4, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLnDXPvJ4WSo"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5UubCzK4WSo"
      },
      "source": [
        "### 미니 튜토리얼\n",
        "- 아래 튜토리얼을 따라하며 Skip-Gram 모델의 `forward` 및 `loss` 연산 방식을 이해하자\n",
        "- Reference\n",
        "    - [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "    - [torch bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n",
        "    - [Skip-Gram negative sampling loss function 설명 영문 블로그](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#Derivation-of-Cost-Function-in-Negative-Sampling)\n",
        "    - [Skip-Gram negative sampling loss function 설명 한글 블로그](https://reniew.github.io/22/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:46.954048Z",
          "start_time": "2022-02-19T12:51:46.951529Z"
        },
        "id": "IAR68hsY4WSo"
      },
      "outputs": [],
      "source": [
        "# hyper parameter example\n",
        "emb_size = 30000 # vocab size\n",
        "emb_dimension = 300 # word embedding 차원\n",
        "n_neg_sample = 5\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.340056Z",
          "start_time": "2022-02-19T12:51:47.300999Z"
        },
        "id": "zzOsVUn94WSo"
      },
      "outputs": [],
      "source": [
        "# 1. Embedding Matrix와 Context Matrix를 생성\n",
        "u_embedding = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device)\n",
        "v_embedding = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.352240Z",
          "start_time": "2022-02-19T12:51:49.341437Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7J_ADc44WSo",
        "outputId": "41d82321-be64-4fba-b0f5-a6e027f914d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target word idx : tensor([24460, 10634,  2864, 23952,  3320, 15187, 19625, 26546, 27339,  3920,\n",
            "        25847,  6023,  5055,  7070,  6291, 10245, 15926,   641, 20178,  4565,\n",
            "         4784, 26715, 16955, 28742, 17947, 19774,  8065, 22605,  3061, 28965,\n",
            "         3056, 17963]) Pos context word idx : tensor([23224,  5636, 23712,  5234,  3991, 17897, 25123, 17938, 19634, 24228,\n",
            "          693,   799, 25457,  1308, 28935, 25696,  5601, 23878,  8312,  1292,\n",
            "        21380, 16974,  9318,  9578, 12915, 29271, 26465, 20572,  2362, 25929,\n",
            "        19754, 29080]) Neg context word idx : [[ 396 2689  413   31  276]\n",
            " [ 472 4276 5467 3444  785]\n",
            " [4101  274  835  337 1200]\n",
            " [ 212 1107 3161  812  137]\n",
            " [  17 3072 1731    3 5058]\n",
            " [ 482  229  302 3434  270]\n",
            " [2087  667  486 1932 1762]\n",
            " [ 313 2200 4417   34 4620]\n",
            " [3050   98   43 1689  122]\n",
            " [ 520 2005  222   42 2882]\n",
            " [ 774  334  884 4116 4730]\n",
            " [ 399 1141 2300  188  675]\n",
            " [2429  129 1052  190   27]\n",
            " [ 268 3419 1728 3269   14]\n",
            " [ 894   52 4759  226 1320]\n",
            " [3959  513 3182 3678  467]\n",
            " [2064  516   36  738  407]\n",
            " [2724 1191   32  936  122]\n",
            " [ 739 3362  958 4412   48]\n",
            " [1676  270 1356 3374 2165]\n",
            " [1573  831  603 5616  795]\n",
            " [2820   25 4414  588   26]\n",
            " [   7 2612 1129   76 4623]\n",
            " [1145  456 4709 5379  361]\n",
            " [3714 1522  644  436 5248]\n",
            " [ 874 2670 2196 4820  849]\n",
            " [2365   53 1224 2353 1089]\n",
            " [   9  433 2852 5555 3452]\n",
            " [  52 4684   98 1826 2180]\n",
            " [5032 1754   79  761 1796]\n",
            " [   5 1800 2680  486 2300]\n",
            " [3633 5290 4932  117   47]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. wid(단어 인덱스)를 임의로 생성\n",
        "pos_u = torch.randint(high = emb_size, size = (batch_size,))\n",
        "pos_v = torch.randint(high = emb_size, size = (batch_size,))\n",
        "neg_v = get_neg_v_negative_sampling(batch_size, n_neg_sample)\n",
        "print(f\"Target word idx : {pos_u} Pos context word idx : {pos_v} Neg context word idx : {neg_v}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.364020Z",
          "start_time": "2022-02-19T12:51:49.353486Z"
        },
        "id": "4iEG0nCZ4WSo"
      },
      "outputs": [],
      "source": [
        "# 3. tensor로 변환\n",
        "pos_u = Variable(torch.LongTensor(pos_u)).to(device)\n",
        "pos_v = Variable(torch.LongTensor(pos_v)).to(device)\n",
        "neg_v = Variable(torch.LongTensor(neg_v)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:51.391896Z",
          "start_time": "2022-02-19T12:51:51.387084Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqbNbajG4WSo",
        "outputId": "10f4812c-c8e4-4c11-bbab-248616a2385f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of pos_u embedding : torch.Size([32, 300])\n",
            " shape of pos_v embedding : torch.Size([32, 300])\n",
            " shape of neg_v embedding : torch.Size([32, 5, 300])\n"
          ]
        }
      ],
      "source": [
        "# 4. wid로 각각의 embedding matrix에서 word embedding 값을 가져오기\n",
        "pos_u = u_embedding(pos_u)\n",
        "pos_v = v_embedding(pos_v)\n",
        "neg_v = v_embedding(neg_v)\n",
        "print(f\"shape of pos_u embedding : {pos_u.shape}\\n shape of pos_v embedding : {pos_v.shape}\\n shape of neg_v embedding : {neg_v.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:53.121477Z",
          "start_time": "2022-02-19T12:51:52.646148Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDWUrSwo4WSo",
        "outputId": "fe99394e-e313-4fe2-a47e-04341b634f2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of pos logits : torch.Size([32])\n",
            "\n",
            "shape of logits : torch.Size([32, 5])\n"
          ]
        }
      ],
      "source": [
        "# 5. dot product \n",
        "pos_score = torch.mul(pos_u, pos_v) # 행렬 element-wise 곱\n",
        "pos_score = torch.sum(pos_score, dim=1)\n",
        "print(f\"shape of pos logits : {pos_score.shape}\\n\")\n",
        "\n",
        "neg_score = torch.bmm(neg_v, pos_u.unsqueeze(dim=2)).squeeze() #unsqueeze로 차원 생성\n",
        "print(f\"shape of logits : {neg_score.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:53.670418Z",
          "start_time": "2022-02-19T12:51:53.665671Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adOpcoL54WSo",
        "outputId": "74ac29ee-ce2b-4e54-9182-be0f5189f49c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pos logits : -241.4199676513672\n",
            "neg logits : -1072.044189453125\n",
            "Loss : 1313.464111328125\n"
          ]
        }
      ],
      "source": [
        "# 6. loss 구하기\n",
        "pos_score = F.logsigmoid(pos_score)\n",
        "neg_score = F.logsigmoid(-1*neg_score) # negative의 logit은 minimize 하기 위해 -1 곱함\n",
        "print(f\"pos logits : {pos_score.sum()}\")\n",
        "print(f\"neg logits : {neg_score.sum()}\")\n",
        "loss = -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
        "print(f\"Loss : {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muEceOGZ4WSo"
      },
      "source": [
        "### Skip-gram 클래스 구현\n",
        "- Skip-Gram 방식으로 단어 embedding을 학습하는 `SkipGram` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()` 함수) 입력 매개변수\n",
        "        - `vocab_size` : 사전내 단어 개수\n",
        "        - `emb_dimension` : 엠베딩 크기\n",
        "        - `device` : 연산 장치 종류\n",
        "    - 생성자에서 생성해야할 변수 \n",
        "        - `vocab_size` : 사전내 단어 개수\n",
        "        - `emb_dimension` : 엠베딩 크기\n",
        "        - `u_embedding` : (vocab_size, emb_dimension) 엠베딩 메트릭스 (target_word)\n",
        "        - `v_embedding` : (vocab_size, emb_dimension) 엠베딩 메트릭스 (context_word)\n",
        "    - 메소드\n",
        "        - `init_embedding()` (제공됨)\n",
        "            - 엠베딩 메트릭스 값을 초기화\n",
        "        - `forward()`\n",
        "            - 위 튜토리얼과 같이 dot product를 수행한 후 score를 생성\n",
        "            - loss를 반환 (loss 설명 추가)\n",
        "        - `save_emedding()` (제공됨)\n",
        "            - `u_embedding`의 단어 엠베딩 값을 단어 별로 파일에 저장\n",
        "    - 주의 사항     \n",
        "        - `nn.Module`를 부모 클래스로 상속 받음 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:15.731306Z",
          "start_time": "2022-02-19T14:34:15.721129Z"
        },
        "id": "pnmMamP44WSo"
      },
      "outputs": [],
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size:int, emb_dimension:int, device:str):\n",
        "        super(SkipGram, self).__init__()\n",
        "        # 변수 선언\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        #u : target, v : context\n",
        "        self.u_embedding = nn.Embedding(vocab_size, emb_dimension, sparse=True).to(device)\n",
        "        self.v_embedding = nn.Embedding(vocab_size, emb_dimension, sparse=True).to(device)\n",
        "        self.init_embedding()\n",
        "    \n",
        "    \n",
        "    def init_embedding(self):\n",
        "        \"\"\"\n",
        "        u_embedding과 v_embedding 메트릭스 값을 초기화\n",
        "        \"\"\"\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        self.u_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embedding.weight.data.uniform_(-0, 0)\n",
        "    \n",
        "    \n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        \"\"\"\n",
        "        dot product를 수행한 후 score를 생성\n",
        "        loss 반환\n",
        "        \"\"\"    \n",
        "            \n",
        "        # 각각의 embedding matrix에서 word embedding 값을 가져오기\n",
        "        pos_u = self.u_embedding(pos_u)\n",
        "        pos_v = self.v_embedding(pos_v)\n",
        "        neg_v = self.v_embedding(neg_v)\n",
        "        \n",
        "        # dot product \n",
        "        pos_score = torch.mul(pos_u, pos_v) # 행렬 element-wise 곱\n",
        "        pos_score = torch.sum(pos_score, dim=1)         \n",
        "        neg_score = torch.bmm(neg_v, pos_u.unsqueeze(dim=2)).squeeze() #unsqueeze로 차원 생성\n",
        "        \n",
        "        # loss 구하기\n",
        "        # negative-sampling으로 binary-classification이므로 sigmoid\n",
        "        pos_score = F.logsigmoid(pos_score)\n",
        "        neg_score = F.logsigmoid(-1*neg_score) # negative의 logit은 minimize 하기 위해 -1 곱함\n",
        "        loss = -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def save_embedding(self, id2word, file_name, use_cuda):\n",
        "        \"\"\"\n",
        "        'file_name' 위치에 word와 word_embedding을 line-by로 저장\n",
        "        파일의 첫 줄은 '단어 개수' 그리고 '단어 embedding 사이즈' 값을 입력해야 함\n",
        "        \"\"\"\n",
        "        if use_cuda: # parameter를 gpu 메모리에서 cpu 메모리로 옮김\n",
        "            embedding = self.u_embedding.weight.cpu().data.numpy()\n",
        "        else:\n",
        "            embedding = self.u_embedding.weight.data.numpy()\n",
        "\n",
        "        with open(file_name, 'w') as writer:\n",
        "            # 파일의 첫 줄은 '단어 개수' 그리고 '단어 embedding 사이즈' 값을 입력해야 함\n",
        "            writer.write(f\"{len(id2word)} {embedding.shape[-1]}\\n\")\n",
        "            \n",
        "            for wid, word in id2word.items():\n",
        "                e = embedding[wid]\n",
        "                e = \" \".join([str(e_) for e_ in e])\n",
        "                writer.write(f\"{word} {e}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqqMo0zL4WSo"
      },
      "source": [
        "## Advanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSWd5gV24WSp"
      },
      "source": [
        "### Skip-Gram 방식의  Word2Vec 클래스 구현\n",
        "- Skip-Gram 방식으로 단어 embedding을 학습하는 `Word2Vec` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()`) 입력 매개 변수\n",
        "        - `input_file` : 학습할 문서 리스트\n",
        "        - `output_file_name` : 학습된 word embedding을 저장할 파일 위치\n",
        "        - `device` : 연상 장치 종류\n",
        "        - `emb_dimension` : word embedding 차원\n",
        "        - `batch_size` : 학습 배치 사이즈\n",
        "        - `window_size` : skip-gram 윈도우 사이즈 (context word 개수를 결정)\n",
        "        - `n_neg_sample` : negative sample 개수\n",
        "        - `iteration` : 학습 반복 횟수\n",
        "        - `lr` : learning rate\n",
        "        - `min_count` : 사전에 추가될 단어의 최소 등장 빈도\n",
        "    - 생성자에서 생성해야 할 변수 \n",
        "        - `docs` : 학습할 문서 리스트\n",
        "        - `output_file_name` : 학습된 word embedding을 저장할 파일 위치\n",
        "        - `word2count`, `word2id`, `id2word` : 위에서 구현한 `make_vocab()` 함수의 반환 값\n",
        "        - `device` : 연산 장치 종류\n",
        "        - `emb_size` : vocab의 (unique한) 단어 종류 \n",
        "        - `emb_dimension` : word embedding 차원\n",
        "        - `batch_size` : 학습 배치 사이즈\n",
        "        - `window_size` : skip-gram 윈도우 사이즈 (context word 개수를 결정)\n",
        "        - `n_neg_sample` : negative sample 개수\n",
        "        - `iteration` : 학습 반복 횟수\n",
        "        - `lr` : learning rate\n",
        "        - `model` : `SkipGram` 클래스의 인스턴스\n",
        "        - `optimizer` : `SGD` 클래스의 인스턴스\n",
        "    - 메소드\n",
        "        - `train()`\n",
        "            - 입력 매개변수 \n",
        "                - `train_dataloader`\n",
        "            - Iteration 횟수만큼 input_file 학습 데이터를 학습한다. 매 epoch마다 for loop 돌면서 batch 단위 학습 데이터를 skip gram 모델에 학습함. 학습이 끝나면 word embedding을 output_file_name 파일에 저장.\n",
        "- Reference\n",
        "    - [Optimizer - SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:20.503555Z",
          "start_time": "2022-02-19T14:34:20.491585Z"
        },
        "id": "Td-GQrqI4WSp"
      },
      "outputs": [],
      "source": [
        "class Word2Vec:\n",
        "    def __init__(self, \n",
        "                input_file: List[str],\n",
        "                output_file_name: str,\n",
        "                 device: str,\n",
        "                 emb_dimension=300,\n",
        "                 batch_size = 64,\n",
        "                 window_size=5,\n",
        "                 n_neg_sample = 5,\n",
        "                 iteration=1,\n",
        "                 lr = 0.02,\n",
        "                 min_count=5):\n",
        "        # 변수 선언\n",
        "        self.docs = input_file\n",
        "        self.output_file_name = output_file_name\n",
        "        self.word2count, self.word2id, self.id2word = make_vocab(self.docs, min_count)\n",
        "        self.device = device\n",
        "        self.emb_size = len(self.word2id)\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.batch_size = batch_size\n",
        "        self.window_size = window_size\n",
        "        self.n_neg_sample = n_neg_sample\n",
        "        self.iteration = iteration\n",
        "        self.lr = lr\n",
        "        self.model = SkipGram(self.emb_size, self.emb_dimension, self.device)\n",
        "        self.optimizer = torch.optim.SGD(self.model.parameters(), self.lr) # torch.optim.SGD 클래스 사용\n",
        "\n",
        "        # train() 함수에서 만든 임베딩 결과 파일들을 저장할 폴더 생성 (os.makedirs 사용)\n",
        "        if not os.path.isdir(self.output_file_name):\n",
        "            os.makedirs(self.output_file_name)\n",
        " \n",
        "    def train(self, train_dataloader):\n",
        "        \n",
        "        # lr 값을 조절하는 스케줄러 인스턴스 변수를 생성\n",
        "        # SGD, training_steps 입력\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer = self.optimizer,\n",
        "            num_warmup_steps=0,\n",
        "            num_training_steps = len(train_dataloader) * self.iteration\n",
        "        )\n",
        "        \n",
        "        for epoch in range(self.iteration):\n",
        "            \n",
        "            print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "            print(f\"*****Epoch {epoch} Total Step {len(train_dataloader)}*****\")\n",
        "            total_loss, batch_loss, batch_step = 0,0,0\n",
        "\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                batch_step+=1\n",
        "\n",
        "                pos_u, pos_v = batch\n",
        "                # negative data 생성\n",
        "                neg_v = get_neg_v_negative_sampling(pos_u.shape[0], self.n_neg_sample)\n",
        "                \n",
        "                # 데이터를 tensor화 & device 설정\n",
        "                pos_u = Variable(torch.LongTensor(pos_u)).to(device)\n",
        "                pos_v = Variable(torch.LongTensor(pos_v)).to(device)\n",
        "                neg_v = Variable(torch.LongTensor(neg_v)).to(device)\n",
        "\n",
        "                # model의 gradient 초기화\n",
        "                self.model.zero_grad()\n",
        "                # optimizer의 gradient 초기화\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                loss = self.model.forward(pos_u, pos_v, neg_v)\n",
        "                # loss 저장\n",
        "                batch_loss += loss.item()\n",
        "                total_loss += loss.item()\n",
        "                # loss 업데이트\n",
        "                loss.backward()\n",
        "                # optimizer 업데이트\n",
        "                self.optimizer.step()\n",
        "                # scheduler 업데이트\n",
        "                self.scheduler.step()\n",
        "\n",
        "                \n",
        "                if (step%500 == 0) and (step!=0):\n",
        "                    print(f\"Step: {step} Loss: {batch_loss/batch_step:.4f} lr: {self.optimizer.param_groups[0]['lr']:.4f}\")\n",
        "                    # 변수 초기화    \n",
        "                    batch_loss, batch_step = 0,0\n",
        "            \n",
        "            print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "            print(f\"*****Epoch {epoch} Train Finished*****\\n\")\n",
        "            \n",
        "            print(f\"*****Epoch {epoch} Saving Embedding...*****\")\n",
        "            self.model.save_embedding(self.id2word, os.path.join(self.output_file_name, f'w2v_{epoch}.txt'), True if 'cuda' in self.device.type else False)\n",
        "            print(f\"*****Epoch {epoch} Embedding Saved at {os.path.join(self.output_file_name, f'w2v_{epoch}.txt')}*****\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:29.561892Z",
          "start_time": "2022-02-19T14:34:26.103659Z"
        },
        "id": "Ywx9R8n24WSp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 14417.68it/s]\n"
          ]
        }
      ],
      "source": [
        "output_file = os.path.join(\".\", \"word2vec_wiki\")\n",
        "# Word2Vec 클래스의 인스턴스 생성\n",
        "w2v = Word2Vec(docs, output_file, device, n_neg_sample=10, iteration=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:34.615469Z",
          "start_time": "2022-02-19T14:34:34.055502Z"
        },
        "id": "ufBxjKxN4WSp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24973"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 학습 데이터 셋 및 데이터 로더 생성 (위에서 생성한 w2v의 attribute들을 argument에 적절히 넣기)\n",
        "dataset = CustomDataset(docs, w2v.word2id, w2v.window_size)\n",
        "train_dataloader = DataLoader(dataset, w2v.batch_size, shuffle = True, generator=torch.manual_seed(seed))\n",
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:45:38.362817Z",
          "start_time": "2022-02-19T14:34:37.382371Z"
        },
        "id": "9JBUrUJ34WSp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*****Epoch 0 Train Start*****\n",
            "*****Epoch 0 Total Step 24973*****\n",
            "Step: 500 Loss: 487.8718 lr: 0.0199\n",
            "Step: 1000 Loss: 464.9959 lr: 0.0197\n",
            "Step: 1500 Loss: 396.6303 lr: 0.0196\n",
            "Step: 2000 Loss: 345.0251 lr: 0.0195\n",
            "Step: 2500 Loss: 307.8862 lr: 0.0193\n",
            "Step: 3000 Loss: 280.6200 lr: 0.0192\n",
            "Step: 3500 Loss: 261.8244 lr: 0.0191\n",
            "Step: 4000 Loss: 248.4241 lr: 0.0189\n",
            "Step: 4500 Loss: 238.0951 lr: 0.0188\n",
            "Step: 5000 Loss: 230.0892 lr: 0.0187\n",
            "Step: 5500 Loss: 224.0149 lr: 0.0185\n",
            "Step: 6000 Loss: 220.0730 lr: 0.0184\n",
            "Step: 6500 Loss: 215.4862 lr: 0.0183\n",
            "Step: 7000 Loss: 212.1368 lr: 0.0181\n",
            "Step: 7500 Loss: 210.0763 lr: 0.0180\n",
            "Step: 8000 Loss: 207.6408 lr: 0.0179\n",
            "Step: 8500 Loss: 206.2505 lr: 0.0177\n",
            "Step: 9000 Loss: 204.4827 lr: 0.0176\n",
            "Step: 9500 Loss: 203.0423 lr: 0.0175\n",
            "Step: 10000 Loss: 201.5071 lr: 0.0173\n",
            "Step: 10500 Loss: 200.2520 lr: 0.0172\n",
            "Step: 11000 Loss: 199.3880 lr: 0.0171\n",
            "Step: 11500 Loss: 198.7036 lr: 0.0169\n",
            "Step: 12000 Loss: 198.0938 lr: 0.0168\n",
            "Step: 12500 Loss: 197.5546 lr: 0.0167\n",
            "Step: 13000 Loss: 197.5362 lr: 0.0165\n",
            "Step: 13500 Loss: 196.4579 lr: 0.0164\n",
            "Step: 14000 Loss: 195.8395 lr: 0.0163\n",
            "Step: 14500 Loss: 194.6345 lr: 0.0161\n",
            "Step: 15000 Loss: 194.5441 lr: 0.0160\n",
            "Step: 15500 Loss: 194.4582 lr: 0.0159\n",
            "Step: 16000 Loss: 193.8538 lr: 0.0157\n",
            "Step: 16500 Loss: 193.4801 lr: 0.0156\n",
            "Step: 17000 Loss: 193.1838 lr: 0.0155\n",
            "Step: 17500 Loss: 192.9737 lr: 0.0153\n",
            "Step: 18000 Loss: 192.2201 lr: 0.0152\n",
            "Step: 18500 Loss: 192.0314 lr: 0.0151\n",
            "Step: 19000 Loss: 191.9867 lr: 0.0149\n",
            "Step: 19500 Loss: 191.8327 lr: 0.0148\n",
            "Step: 20000 Loss: 191.1634 lr: 0.0147\n",
            "Step: 20500 Loss: 190.9906 lr: 0.0145\n",
            "Step: 21000 Loss: 190.6169 lr: 0.0144\n",
            "Step: 21500 Loss: 190.6246 lr: 0.0143\n",
            "Step: 22000 Loss: 190.4835 lr: 0.0141\n",
            "Step: 22500 Loss: 189.9920 lr: 0.0140\n",
            "Step: 23000 Loss: 189.8468 lr: 0.0139\n",
            "Step: 23500 Loss: 189.0199 lr: 0.0137\n",
            "Step: 24000 Loss: 189.5411 lr: 0.0136\n",
            "Step: 24500 Loss: 188.6041 lr: 0.0135\n",
            "Epoch 0 Total Mean Loss : 223.3409\n",
            "*****Epoch 0 Train Finished*****\n",
            "\n",
            "*****Epoch 0 Saving Embedding...*****\n",
            "*****Epoch 0 Embedding Saved at ./word2vec_wiki/w2v_0.txt*****\n",
            "\n",
            "*****Epoch 1 Train Start*****\n",
            "*****Epoch 1 Total Step 24973*****\n",
            "Step: 500 Loss: 188.5256 lr: 0.0132\n",
            "Step: 1000 Loss: 188.3006 lr: 0.0131\n",
            "Step: 1500 Loss: 188.1285 lr: 0.0129\n",
            "Step: 2000 Loss: 187.1853 lr: 0.0128\n",
            "Step: 2500 Loss: 187.6049 lr: 0.0127\n",
            "Step: 3000 Loss: 187.6127 lr: 0.0125\n",
            "Step: 3500 Loss: 187.1266 lr: 0.0124\n",
            "Step: 4000 Loss: 186.5213 lr: 0.0123\n",
            "Step: 4500 Loss: 186.3594 lr: 0.0121\n",
            "Step: 5000 Loss: 186.5930 lr: 0.0120\n",
            "Step: 5500 Loss: 186.9393 lr: 0.0119\n",
            "Step: 6000 Loss: 185.9900 lr: 0.0117\n",
            "Step: 6500 Loss: 186.0772 lr: 0.0116\n",
            "Step: 7000 Loss: 185.8366 lr: 0.0115\n",
            "Step: 7500 Loss: 185.9017 lr: 0.0113\n",
            "Step: 8000 Loss: 185.9270 lr: 0.0112\n",
            "Step: 8500 Loss: 185.7824 lr: 0.0111\n",
            "Step: 9000 Loss: 185.5326 lr: 0.0109\n",
            "Step: 9500 Loss: 185.4851 lr: 0.0108\n",
            "Step: 10000 Loss: 184.8641 lr: 0.0107\n",
            "Step: 10500 Loss: 185.4182 lr: 0.0105\n",
            "Step: 11000 Loss: 185.1132 lr: 0.0104\n",
            "Step: 11500 Loss: 185.0090 lr: 0.0103\n",
            "Step: 12000 Loss: 184.8590 lr: 0.0101\n",
            "Step: 12500 Loss: 184.9204 lr: 0.0100\n",
            "Step: 13000 Loss: 184.1245 lr: 0.0099\n",
            "Step: 13500 Loss: 185.0649 lr: 0.0097\n",
            "Step: 14000 Loss: 183.8982 lr: 0.0096\n",
            "Step: 14500 Loss: 183.8917 lr: 0.0095\n",
            "Step: 15000 Loss: 184.0332 lr: 0.0093\n",
            "Step: 15500 Loss: 184.2583 lr: 0.0092\n",
            "Step: 16000 Loss: 184.0469 lr: 0.0091\n",
            "Step: 16500 Loss: 183.6268 lr: 0.0089\n",
            "Step: 17000 Loss: 183.1811 lr: 0.0088\n",
            "Step: 17500 Loss: 182.6879 lr: 0.0087\n",
            "Step: 18000 Loss: 182.6427 lr: 0.0085\n",
            "Step: 18500 Loss: 182.9339 lr: 0.0084\n",
            "Step: 19000 Loss: 182.7271 lr: 0.0083\n",
            "Step: 19500 Loss: 182.7690 lr: 0.0081\n",
            "Step: 20000 Loss: 182.5059 lr: 0.0080\n",
            "Step: 20500 Loss: 182.7376 lr: 0.0079\n",
            "Step: 21000 Loss: 182.2607 lr: 0.0077\n",
            "Step: 21500 Loss: 181.9352 lr: 0.0076\n",
            "Step: 22000 Loss: 182.1434 lr: 0.0075\n",
            "Step: 22500 Loss: 182.3021 lr: 0.0073\n",
            "Step: 23000 Loss: 181.2919 lr: 0.0072\n",
            "Step: 23500 Loss: 182.2649 lr: 0.0071\n",
            "Step: 24000 Loss: 182.1214 lr: 0.0069\n",
            "Step: 24500 Loss: 181.1266 lr: 0.0068\n",
            "Epoch 1 Total Mean Loss : 184.5978\n",
            "*****Epoch 1 Train Finished*****\n",
            "\n",
            "*****Epoch 1 Saving Embedding...*****\n",
            "*****Epoch 1 Embedding Saved at ./word2vec_wiki/w2v_1.txt*****\n",
            "\n",
            "*****Epoch 2 Train Start*****\n",
            "*****Epoch 2 Total Step 24973*****\n",
            "Step: 500 Loss: 180.7371 lr: 0.0065\n",
            "Step: 1000 Loss: 180.7019 lr: 0.0064\n",
            "Step: 1500 Loss: 180.5051 lr: 0.0063\n",
            "Step: 2000 Loss: 180.4574 lr: 0.0061\n",
            "Step: 2500 Loss: 180.4160 lr: 0.0060\n",
            "Step: 3000 Loss: 180.6526 lr: 0.0059\n",
            "Step: 3500 Loss: 179.9749 lr: 0.0057\n",
            "Step: 4000 Loss: 179.6769 lr: 0.0056\n",
            "Step: 4500 Loss: 179.9102 lr: 0.0055\n",
            "Step: 5000 Loss: 180.6838 lr: 0.0053\n",
            "Step: 5500 Loss: 180.1735 lr: 0.0052\n",
            "Step: 6000 Loss: 180.1364 lr: 0.0051\n",
            "Step: 6500 Loss: 180.3384 lr: 0.0049\n",
            "Step: 7000 Loss: 180.0152 lr: 0.0048\n",
            "Step: 7500 Loss: 180.3526 lr: 0.0047\n",
            "Step: 8000 Loss: 179.8755 lr: 0.0045\n",
            "Step: 8500 Loss: 180.1980 lr: 0.0044\n",
            "Step: 9000 Loss: 179.7681 lr: 0.0043\n",
            "Step: 9500 Loss: 180.4286 lr: 0.0041\n",
            "Step: 10000 Loss: 180.2158 lr: 0.0040\n",
            "Step: 10500 Loss: 179.2119 lr: 0.0039\n",
            "Step: 11000 Loss: 180.4194 lr: 0.0037\n",
            "Step: 11500 Loss: 179.2991 lr: 0.0036\n",
            "Step: 12000 Loss: 179.4454 lr: 0.0035\n",
            "Step: 12500 Loss: 179.8500 lr: 0.0033\n",
            "Step: 13000 Loss: 179.0562 lr: 0.0032\n",
            "Step: 13500 Loss: 179.8465 lr: 0.0031\n",
            "Step: 14000 Loss: 179.7595 lr: 0.0029\n",
            "Step: 14500 Loss: 179.0957 lr: 0.0028\n",
            "Step: 15000 Loss: 179.3185 lr: 0.0027\n",
            "Step: 15500 Loss: 179.6440 lr: 0.0025\n",
            "Step: 16000 Loss: 178.9962 lr: 0.0024\n",
            "Step: 16500 Loss: 178.9084 lr: 0.0023\n",
            "Step: 17000 Loss: 179.1538 lr: 0.0021\n",
            "Step: 17500 Loss: 178.8370 lr: 0.0020\n",
            "Step: 18000 Loss: 178.6305 lr: 0.0019\n",
            "Step: 18500 Loss: 178.9303 lr: 0.0017\n",
            "Step: 19000 Loss: 179.3035 lr: 0.0016\n",
            "Step: 19500 Loss: 178.7312 lr: 0.0015\n",
            "Step: 20000 Loss: 178.8239 lr: 0.0013\n",
            "Step: 20500 Loss: 179.0765 lr: 0.0012\n",
            "Step: 21000 Loss: 178.5655 lr: 0.0011\n",
            "Step: 21500 Loss: 179.1521 lr: 0.0009\n",
            "Step: 22000 Loss: 179.1241 lr: 0.0008\n",
            "Step: 22500 Loss: 179.2281 lr: 0.0007\n",
            "Step: 23000 Loss: 178.5648 lr: 0.0005\n",
            "Step: 23500 Loss: 179.6744 lr: 0.0004\n",
            "Step: 24000 Loss: 178.7099 lr: 0.0003\n",
            "Step: 24500 Loss: 178.2669 lr: 0.0001\n",
            "Epoch 2 Total Mean Loss : 179.5916\n",
            "*****Epoch 2 Train Finished*****\n",
            "\n",
            "*****Epoch 2 Saving Embedding...*****\n",
            "*****Epoch 2 Embedding Saved at ./word2vec_wiki/w2v_2.txt*****\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 학습\n",
        "w2v.train(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uTIm4vJ4WSp"
      },
      "source": [
        "### 유사한 단어 확인\n",
        "- 사전에 존재하는 단어들과 유사한 단어를 검색해보자. Gensim 패키지는 유사 단어 외에도 단어간의 유사도를 계산하는 여러 함수를 제공한다. 실험을 통해 word2vec의 한계점을 발견했다면 아래에 markdown으로 작성해보자. \n",
        "- [Gensim 패키지 document](https://radimrehurek.com/gensim/models/keyedvectors.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:47:59.605389Z",
          "start_time": "2022-02-19T14:47:59.368925Z"
        },
        "id": "AKpBuVlP4WSp"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:49:06.590460Z",
          "start_time": "2022-02-19T14:49:05.174241Z"
        },
        "id": "AWTCodimsAq8"
      },
      "outputs": [],
      "source": [
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format('word2vec_wiki/w2v_1.txt', binary=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Positive에 미국, 영국만 입력했을 때는 미국, 영국을 '나라'로 인식.   \n",
        "Negative인 중국이 추가됐을 때는 제국주의 열강으로 인식하는 거 같음.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:49:11.324372Z",
          "start_time": "2022-02-19T14:49:11.315429Z"
        },
        "id": "MLMh_evrsAq9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('러시아', 0.9313470125198364),\n",
              " ('네덜란드', 0.9282569885253906),\n",
              " ('인민공화국', 0.9255377054214478),\n",
              " ('독일', 0.9237388372421265),\n",
              " ('제국', 0.9225962162017822),\n",
              " ('연방', 0.9219838976860046),\n",
              " ('학회', 0.9196910262107849),\n",
              " ('오스트리아', 0.9192821979522705),\n",
              " ('군인', 0.9156433939933777),\n",
              " ('혁명', 0.9148563742637634)]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_vectors.most_similar(positive=['미국', '영국'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('제국', 0.882709801197052),\n",
              " ('전쟁', 0.8668127655982971),\n",
              " ('스페인', 0.8579126000404358),\n",
              " ('러시아', 0.8495883941650391),\n",
              " ('음반', 0.8392270803451538),\n",
              " ('조선', 0.8358479738235474),\n",
              " ('왕국', 0.8313726186752319),\n",
              " ('키프로스', 0.8302729725837708),\n",
              " ('연방', 0.8299248218536377),\n",
              " ('오스트리아', 0.8192951679229736)]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_vectors.most_similar(positive=['미국', '영국'], negative = ['중국'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"Key '된장국' not present\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_40557/2217326928.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'미국'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'영국'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'된장국'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    771\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \"\"\"\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key '된장국' not present\""
          ]
        }
      ],
      "source": [
        "word_vectors.most_similar(positive=['미국', '영국'], negative = ['된장국'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8lc8NQe4cT2"
      },
      "source": [
        "word2vec의 한계점은?\n",
        "- corpus에 포함되지 않은 단어는 word2vec representation을 얻을 수 없다는 단점 존재.\n",
        "- window 범위 안의 단어들로 context를 완전히 표현하긴 힘듦."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Week3_1_assignment.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "70a8288ab958a215e8d75c20ae32722b3e32ac0386630f0daa1847295c58910f"
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
