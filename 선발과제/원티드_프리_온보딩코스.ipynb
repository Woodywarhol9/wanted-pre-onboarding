{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTikv1tuEgXS"
      },
      "source": [
        "### **문제 1) Tokenizer 생성하기**\n",
        "\n",
        "**1-1. `preprocessing()`**\n",
        "\n",
        "텍스트 전처리를 하는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- output: 각 문장을 토큰화한 결과로, nested list 형태입니다. ex) [['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]\n",
        "- 조건 1: 입력된 문장에 대해서 소문자로의 변환과 특수문자 제거를 수행합니다.\n",
        "- 조건 2: 토큰화는 white space 단위로 수행합니다.\n",
        "    \n",
        "    \n",
        "\n",
        "**1-2. `fit()`**\n",
        "\n",
        "어휘 사전을 구축하는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- 조건 1: 위에서 만든 `preprocessing` 함수를 이용하여 각 문장에 대해 토큰화를 수행합니다.\n",
        "- 조건 2: 각각의 토큰을 정수 인덱싱 하기 위한 어휘 사전(`self.word_dict`)을 생성합니다.\n",
        "    - 주어진 코드에 있는 `self.word_dict`를 활용합니다.\n",
        "    \n",
        "\n",
        "**1-3. `transform()`**\n",
        "\n",
        "어휘 사전을 활용하여 입력 문장을 정수 인덱싱하는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- output: 각 문장의 정수 인덱싱으로, nested list 형태입니다. ex) [[1, 2, 3, 4], [1, 5, 6]]\n",
        "- 조건 1: 어휘 사전(`self.word_dict`)에 없는 단어는 'oov'의 index로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "pFGWyAraJ5W1"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "dzfrBShT9BeR"
      },
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "    \"\"\"\n",
        "    문제 1-1\n",
        "    \"\"\"\n",
        "    for sentence in sequences:\n",
        "      # 소문자 변환\n",
        "      temp = sentence.lower() \n",
        "      # 특수 문자 제거\n",
        "      temp = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]','', temp) \n",
        "      # 공백 기준 분리\n",
        "      temp = temp.split(\" \") \n",
        "      result.append(temp)\n",
        "    \n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    '''\n",
        "    문제 1-2.\n",
        "    '''\n",
        "    # 전처리\n",
        "    tokens = self.preprocessing(sequences) \n",
        "    # 인덱스 설정\n",
        "    idx = len(self.word_dict)\n",
        "    for tok in tokens:  \n",
        "      for word in tok:\n",
        "        # word_dict에 없을 경우 추가\n",
        "        if word not in self.word_dict: \n",
        "          # 단순 출현 순서에 따른 인덱스 부여\n",
        "          self.word_dict[word] = idx \n",
        "          idx += 1\n",
        "\n",
        "    self.fit_checker = True\n",
        "  \n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    tokens = self.preprocessing(sequences) # 전처리\n",
        "\n",
        "    if self.fit_checker:\n",
        "      '''\n",
        "      문제 1-3.\n",
        "      '''\n",
        "      for tok in tokens:\n",
        "        word_to_idx = [] \n",
        "        for word in tok:\n",
        "          if word in self.word_dict: #word_dict에 있으면 word에 해당하는 idx, 없다면 oov \n",
        "            word_to_idx.append(self.word_dict[word]) \n",
        "          else:\n",
        "            word_to_idx.append(self.word_dict[\"oov\"])\n",
        "        result.append(word_to_idx)\n",
        "\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhdVyPFf7ZbT"
      },
      "source": [
        "#### 문제1 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "clun-_27MvQa"
      },
      "outputs": [],
      "source": [
        "test_sentence = [\"I go to school\", \"I LIKE pizza!\", \"she wants to eat pizza~~\"]\n",
        "tok_test = Tokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWMYAPEb9bsF"
      },
      "source": [
        "1-1. preprocessing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ46sHgr8HmB",
        "outputId": "f9f197da-0703-498b-dc5a-48d0ad4131f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['i', 'go', 'to', 'school'], ['i', 'like', 'pizza'], ['she', 'wants', 'to', 'eat', 'pizza']]\n"
          ]
        }
      ],
      "source": [
        "prep_sentence = tok_test.preprocessing(test_sentence)\n",
        "print(prep_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoL3MK1i9eEQ"
      },
      "source": [
        "1-2. fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7zsLTBa7d0G",
        "outputId": "89d2f2cd-dd83-422e-8f76-ba935297b8c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'oov': 0, 'i': 1, 'go': 2, 'to': 3, 'school': 4, 'like': 5, 'pizza': 6, 'she': 7, 'wants': 8, 'eat': 9}\n"
          ]
        }
      ],
      "source": [
        "tok_test.fit(test_sentence)\n",
        "print(tok_test.word_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LtXx8Wc9n9B"
      },
      "source": [
        "1-3. transform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8xuWRJB9aSI",
        "outputId": "65fb7cc1-6a56-4680-a96c-595aca13ab80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 2, 3, 4], [1, 5, 6], [7, 8, 3, 9, 6]]\n"
          ]
        }
      ],
      "source": [
        "trans_sentence = tok_test.transform(test_sentence)\n",
        "print(trans_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE4J1PlI97ck"
      },
      "source": [
        "- 새로운 문장 fit 없이 입력 시 OOV로 인식"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GObnwUQU99SH",
        "outputId": "b7385e27-6270-4ef9-98f8-d0dcd0f54397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0, 0]]\n",
            "{'oov': 0, 'i': 1, 'go': 2, 'to': 3, 'school': 4, 'like': 5, 'pizza': 6, 'she': 7, 'wants': 8, 'eat': 9}\n"
          ]
        }
      ],
      "source": [
        "test_sentence2 = [\"NEW sentence!\"]\n",
        "trans_sentence2 = tok_test.transform(test_sentence2)\n",
        "print(trans_sentence2)\n",
        "print(tok_test.word_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq3ifhShBjTN"
      },
      "source": [
        "- 새로운 문장 fit_transform 결과 새로운 word_dict에 추가됨."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwls67mb_N5u",
        "outputId": "3acfad29-9ac8-4b4b-91b8-d1e70b3a24a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[10, 11]]\n",
            "{'oov': 0, 'i': 1, 'go': 2, 'to': 3, 'school': 4, 'like': 5, 'pizza': 6, 'she': 7, 'wants': 8, 'eat': 9, 'new': 10, 'sentence': 11}\n"
          ]
        }
      ],
      "source": [
        "trans_sentence3 = tok_test.fit_transform(test_sentence2)\n",
        "print(trans_sentence3)\n",
        "print(tok_test.word_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DfgKj5qH6ac"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ0AtlmVEvDV"
      },
      "source": [
        "### **문제 2) TfidfVectorizer 생성하기**\n",
        "\n",
        "**2-1. `fit()`**\n",
        "\n",
        "입력 문장들을 이용해 IDF 행렬을 만드는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- 조건 1: IDF 행렬은 list 형태입니다.\n",
        "    - ex) [토큰1에 대한 IDF 값, 토큰2에 대한 IDF 값, .... ]\n",
        "- 조건 2: IDF 값은 아래 식을 이용해 구합니다.\n",
        "    \n",
        "    $$\n",
        "    idf(d,t)=log_e(\\frac{n}{1+df(d,t)})\n",
        "    $$\n",
        "    \n",
        "    - $df(d,t)$ : 단어 t가 포함된 문장 d의 개수\n",
        "    - $n$ : 입력된 전체 문장 개수\n",
        "- 조건 3: 입력된 문장의 토큰화에는 문제 1에서 만든 Tokenizer를 사용합니다.\n",
        "    \n",
        "    \n",
        "\n",
        "**2-2. `transform()`**\n",
        "\n",
        "입력 문장들을 이용해 TF-IDF 행렬을 만드는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- output : nested list 형태입니다.\n",
        "    \n",
        "    ex) [[tf-idf(1, 1), tf-idf(1, 2), tf-idf(1, 3)], [tf-idf(2, 1), tf-idf(2, 2), tf-idf(2, 3)]]\n",
        "    \n",
        "    |  | 토큰1 | 토큰2 | 토큰3 |\n",
        "    | --- | --- | --- | --- |\n",
        "    | 문장1 | tf-idf(1,1) | tf-idf(1,2) | tf-idf(1,3) |\n",
        "    | 문장2 | tf-idf(2,1) | tf-idf(2,2) | tf-idf(2,3) |\n",
        "- 조건1 : 입력 문장을 이용해 TF 행렬을 만드세요.\n",
        "    - $tf(d, t)$ : 문장 d에 단어 t가 나타난 횟수\n",
        "- 조건2 : 문제 2-1( `fit()`)에서 만든 IDF 행렬과 아래 식을 이용해 TF-IDF 행렬을 만드세요\n",
        "    \n",
        "    $$\n",
        "    tf-idf(d,t) = tf(d,t) \\times idf(d,t)\n",
        "    $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "TFWW509dIr__"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "MV-jLLldEvm2"
      },
      "outputs": [],
      "source": [
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    '''\n",
        "    문제 2-1.\n",
        "    '''\n",
        "    n = len(tokenized)\n",
        "    self.idf_matrix = []\n",
        "    for word in self.tokenizer.word_dict.values():\n",
        "      # oov 토큰 넘어 가기\n",
        "      if word == 0: continue\n",
        "      # 단어별 등장 횟수 확인\n",
        "      df = 0\n",
        "      for tok in tokenized:\n",
        "        if word in tok:\n",
        "          df += 1\n",
        "      self.idf_matrix.append(log(n/(df + 1)))\n",
        "    self.fit_checker = True\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      '''\n",
        "      문제 2-2.\n",
        "      '''\n",
        "      tf_matrix = []\n",
        "      self.tfidf_matrix = []\n",
        "      # 문장별로 확인\n",
        "      for tok in tokenized:\n",
        "        tf_temp = []\n",
        "        # oov 토큰 무시하기 위해서 1부터 시작\n",
        "        for word in range(1 ,len(self.tokenizer.word_dict)):\n",
        "          tf = tok.count(word)\n",
        "          tf_temp.append(tf)\n",
        "        tf_matrix.append(tf_temp)\n",
        "      # tf - idf 계산\n",
        "      tf_matrix_np = np.array(tf_matrix)\n",
        "      idf_matrix_np = np.array(self.idf_matrix)\n",
        "      self.tfidf_matrix = (tf_matrix_np * idf_matrix_np).tolist()\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFnjDeq1KQkk"
      },
      "source": [
        "#### 문제2 확인하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tbm_dIgKuCT"
      },
      "source": [
        "test_sentence = [\"I go to school\", \"I LIKE pizza!\", \"she wants to eat pizza~~\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "HrQHP6RBUF4U"
      },
      "outputs": [],
      "source": [
        "tok_test2 = Tokenizer()\n",
        "tf_test = TfidfVectorizer(tok_test2)\n",
        "tf_test.fit(test_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbGaL2KKHr-k",
        "outputId": "58d6e872-1f71-4bf5-87cd-f42f924b582d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i', 'go', 'to', 'school', 'like', 'pizza', 'she', 'wants', 'eat']"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = list(tok_test2.word_dict.keys())[1:]\n",
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg3-ILB_Km2j"
      },
      "source": [
        "2-1. fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVhOi43FK2JH"
      },
      "source": [
        "- i, to, pizza : 2번씩\n",
        "- 나머지 단어 : 1번씩 출현   \n",
        "따라서 idf 계산 결과 i, to, pizza는 0(log1), 나머지는 0.405(log1.5)가 나오게 된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0, 0.4054651081081644, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644]\n"
          ]
        }
      ],
      "source": [
        "print(tf_test.idf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "OCdEvyjSeoEj",
        "outputId": "08d422e6-4725-457f-d231-a079baa38b91"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>i</th>\n",
              "      <th>go</th>\n",
              "      <th>to</th>\n",
              "      <th>school</th>\n",
              "      <th>like</th>\n",
              "      <th>pizza</th>\n",
              "      <th>she</th>\n",
              "      <th>wants</th>\n",
              "      <th>eat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.405465</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     i        go   to    school      like  pizza       she     wants       eat\n",
              "0  0.0  0.405465  0.0  0.405465  0.405465    0.0  0.405465  0.405465  0.405465"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "idf_result = pd.DataFrame(tf_test.idf_matrix).T\n",
        "idf_result.columns = vocab\n",
        "idf_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fowQmMe_KoDp"
      },
      "source": [
        "2-2. transform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YcapKNkQ_T8",
        "outputId": "9c8fa968-7eae-48d7-e7e0-704d6d790fb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0.0, 0.4054651081081644, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
              " [0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0],\n",
              " [0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.4054651081081644,\n",
              "  0.4054651081081644,\n",
              "  0.4054651081081644]]"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf_idf = tf_test.transform(test_sentence)\n",
        "tf_idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "wppDhM1fGQ8s",
        "outputId": "a9a4f801-2a15-4087-9583-192b5f5662a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>i</th>\n",
              "      <th>go</th>\n",
              "      <th>to</th>\n",
              "      <th>school</th>\n",
              "      <th>like</th>\n",
              "      <th>pizza</th>\n",
              "      <th>she</th>\n",
              "      <th>wants</th>\n",
              "      <th>eat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.405465</td>\n",
              "      <td>0.405465</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     i        go   to    school      like  pizza       she     wants       eat\n",
              "0  0.0  0.405465  0.0  0.405465  0.000000    0.0  0.000000  0.000000  0.000000\n",
              "1  0.0  0.000000  0.0  0.000000  0.405465    0.0  0.000000  0.000000  0.000000\n",
              "2  0.0  0.000000  0.0  0.000000  0.000000    0.0  0.405465  0.405465  0.405465"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf_idf_result = pd.DataFrame(tf_idf, columns = vocab)\n",
        "tf_idf_result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "원티드_프리_온보딩.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
